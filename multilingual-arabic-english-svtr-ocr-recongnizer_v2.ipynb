{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrsFAGE2T6Ui"
   },
   "source": [
    "# SECTION 1: INSTALL PACKAGES AND IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr3P87LS4VhE"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:39:57.568899Z",
     "iopub.status.busy": "2025-09-10T13:39:57.568618Z",
     "iopub.status.idle": "2025-09-10T13:40:04.680667Z",
     "shell.execute_reply": "2025-09-10T13:40:04.679617Z",
     "shell.execute_reply.started": "2025-09-10T13:39:57.568880Z"
    },
    "id": "ssuwe37CT7fU",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install python-bidi arabic-reshaper\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Pachages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:04.683191Z",
     "iopub.status.busy": "2025-09-10T13:40:04.682815Z",
     "iopub.status.idle": "2025-09-10T13:40:04.692275Z",
     "shell.execute_reply": "2025-09-10T13:40:04.691147Z",
     "shell.execute_reply.started": "2025-09-10T13:40:04.683154Z"
    },
    "id": "Oquh7bbqVHLX",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from threading import local\n",
    "from typing import Optional, Tuple, Dict\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as T\n",
    "# from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing as mp\n",
    "\n",
    "import editdistance\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "\n",
    "# Import libraries for handling Arabic text\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9lu5wm_Xq9d"
   },
   "source": [
    "# SECTION 2: CONFIGURATION DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:04.693634Z",
     "iopub.status.busy": "2025-09-10T13:40:04.693331Z",
     "iopub.status.idle": "2025-09-10T13:40:04.720601Z",
     "shell.execute_reply": "2025-09-10T13:40:04.719285Z",
     "shell.execute_reply.started": "2025-09-10T13:40:04.693612Z"
    },
    "id": "P-6OPCZtXsSi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Dataset parameters\n",
    "    'on_colab': False,   # else Kaggle\n",
    "    'use_drive': False, # else Kaggle\n",
    "    # 'dataset_dir': '/content/Arabic_English_OCR_Dataset', # Colab - OLD DATASET\n",
    "    # 'dataset_dir': '/kaggle/input/arabic-english-ocr-dataset/Arabic_English_OCR_Dataset', # Kaggle - OLD DATASET\n",
    "    # 'dataset_dir': '/content/output', # Colab - NEW DATASET\n",
    "    'dataset_dir': '/kaggle/input/arabic-english-ocr-synthatic-dataset-v2/output', # Kaggle - NEW DATASET\n",
    "    'ar_dir': 'ar',\n",
    "    'en_dir': 'en',\n",
    "    'mixed_dir': 'mixed', # New directory for mixed language examples\n",
    "    'labels_file': 'labels.txt',\n",
    "    'max_samples': 100000,  # Total desired samples (e.g., 50k from each dir)\n",
    "    'max_text_length': 32,              # Maximum text sequence length\n",
    "    'permissible_chars': set(\n",
    "                    \" !\\\"#$%&'()*+,-./:;=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz{|}،؛؟٫٬٭\"\n",
    "                    \"0123456789ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيًٌٍَُِّْ٠١٢٣٤٥٦٧٨٩\"\n",
    "                    \"‘’“”\"\n",
    "                ),\n",
    "    'vocab_size': 160,                      # Will be updated after dataset analysis\n",
    "    'train_split_percentage': 0.95,         # Percentage for training set 0.95\n",
    "    'val_split_percentage': 0.025,          # Percentage for validation set 0.025\n",
    "    'test_split_percentage': 0.025,         # Percentage for test set 0.025\n",
    "    'remove_bad_examples': True,            # remove examples with not permissible chars, else remove these characters instead in the text\n",
    "    'do_analysis': True,                   # To do Analysis on the dataset (text --> lengths & chars, images --> shape)\n",
    "\n",
    "    # Image parameters - SVTR standard\n",
    "    'img_height': 64,  # SVTR uses 64 height\n",
    "    'img_width': 256,  # SVTR uses 256 width, (mean width of the dataset 512) (256,384,512)\n",
    "    'channels': 3,\n",
    "\n",
    "    # Model Parameters - SVTR Large\n",
    "    'embed_dims': [128, 256, 384],\n",
    "    'd3': 512,\n",
    "    'heads': [4, 8, 12],                # heads chosen such that embed_dim / num_heads == 32 (nice head dim)\n",
    "    'mlp_ratio': 2,\n",
    "    'dropout_rate': 0.1,\n",
    "    'n_points': 9,\n",
    "    'offset_scale': 4.0,\n",
    "    # ## SMALL ##\n",
    "    'num_blocks': [3, 6, 3],\n",
    "    'pattern': ['L'] * 6 + ['G'] * 6,\n",
    "    'local_type': ['non_overlapping', 'non_overlapping', 'deformable'] * 2 + ['conv'] * 6 ,\n",
    "    'window_sizes': [(7,11)] * 6 + [(3,3)] * 6,\n",
    "    # ## LARGE ##\n",
    "    # 'num_blocks': [3, 12, 3],\n",
    "    # 'pattern': ['L'] * 9 + ['G'] * 9,\n",
    "    # 'local_type': ['non_overlapping', 'non_overlapping', 'deformable'] * 2 + ['deformable'] * 3 + ['conv'] * 9 ,\n",
    "    # 'window_sizes': [(7,11)] * 9 + [(3,3)] * 9,\n",
    "\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 28,         \n",
    "    'early_stopping_patience': 5,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    # TODO: implement warmup\n",
    "    'warmup_epochs': 5,\n",
    "    'gradient_clip': 1.0,\n",
    "\n",
    "    # Augmentation parameters\n",
    "    'augmentation_strength': 0.8, #(0 to 1)\n",
    "    'aug_prob': 0.7,\n",
    "    'rotation_limit': 5,\n",
    "    'blur_limit': 3,\n",
    "    'brightness_limit': 0.2,\n",
    "    'contrast_limit': 0.2,\n",
    "    # 'dataset_mean': [0.485, 0.456, 0.406],       #--> ImageNet Values\n",
    "    # 'dataset_std': [0.229, 0.224, 0.225],        #--> ImageNet Values\n",
    "    # 'dataset_mean': [0.615, 0.617, 0.616],       #--> ar,en dataset Values\n",
    "    # 'dataset_std': [0.271, 0.276, 0.273],        #--> ar,en dataset Values\n",
    "    'dataset_mean': [0.683, 0.691, 0.694],        #--> ar,en,mixed dataset Values \n",
    "    'dataset_std': [0.320, 0.309, 0.301],         #--> ar,en,mixed dataset\n",
    "\n",
    "\n",
    "    # Other parameters\n",
    "    'save_path_directory': './arabic_ocr_checkpoints',\n",
    "    'load_model_path': \"/kaggle/input/svtr_deformable_epoch_26/pytorch/default/1/best_SVTR_deformable_model.pth\", # Old model path\n",
    "    # 'load_model_path': None, # Set to None or update with a new path if you have one\n",
    "    'exclude_head_on_load': False,\n",
    "    'beam_size': 4,  # For beam search during inference\n",
    "    'SEED': 42,\n",
    "    'best_model_filename': 'best_SVTR_deformable_model.pth', # Added: Filename for the best model checkpoint\n",
    "    'regular_checkpoint_frequency': 7,                       # Added: Frequency (in epochs) to save regular checkpoints\n",
    "\n",
    "\n",
    "    # DataLoader parameters\n",
    "    'dataloader_params': {\n",
    "        'batch_size': 32, # (kaggle (LARGE 16, SMALE 32), colab (LARGE 4, SMALE 8)) Increased batch size for visualization\n",
    "        'num_workers': 4 if torch.cuda.is_available() else 0,      # works on kaggle, not colab with gpu\n",
    "        'pin_memory': torch.cuda.is_available(),\n",
    "        'persistent_workers': torch.cuda.is_available()\n",
    "    },\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# SEED\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# set_seed(config['SEED'])\n",
    "\n",
    "# Set the start method to 'spawn' or 'forkserver', for num_workers error\n",
    "# if torch.cuda.is_available():\n",
    "#     mp.set_start_method('spawn', force=True)\n",
    "# else:\n",
    "#     mp.set_start_method('forkserver', force=True)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"Using device: {config['device']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQdwj3sLUXm-"
   },
   "source": [
    "# SECTION 3: DOWNLOAD AND ANALYZE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_jdbN_Ijx4p"
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:04.722886Z",
     "iopub.status.busy": "2025-09-10T13:40:04.722513Z",
     "iopub.status.idle": "2025-09-10T13:40:04.741938Z",
     "shell.execute_reply": "2025-09-10T13:40:04.741104Z",
     "shell.execute_reply.started": "2025-09-10T13:40:04.722855Z"
    },
    "id": "1WHk2-ZQP756",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config['on_colab']:\n",
    "    print(\"====================== On Colab Environment ======================\")\n",
    "    # Check if the dataset directory already exists\n",
    "    if not os.path.exists(config['dataset_dir']) and not config['use_drive']:\n",
    "        from google.colab import files\n",
    "        files.upload()  # Choose the kaggle.json file you downloaded\n",
    "\n",
    "        print(\"Download Dataset From Kaggle: \")\n",
    "        # # Create kaggle directory and move the file\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "        # # Download your dataset\n",
    "        # !kaggle datasets download -d ahmedkamal75/arabic-english-ocr-dataset\n",
    "        !kaggle datasets download -d ahmedkamal75/arabic-english-ocr-synthatic-dataset-v2\n",
    "\n",
    "        # # Unzip the dataset (Kaggle will provide it as a zip)\n",
    "        # !unzip arabic-english-ocr-dataset.zip\n",
    "        !unzip arabic-english-ocr-synthatic-dataset-v2.zip\n",
    "\n",
    "        # !rm -rf arabic-english-ocr-dataset.zip\n",
    "        !rm -rf arabic-english-ocr-synthatic-dataset-v2.zip\n",
    "        !rm -rf kaggle.json\n",
    "\n",
    "        # config['dataset_dir'] = '/content/Arabic_English_OCR_Dataset'\n",
    "        config['dataset_dir'] = '/content/output'\n",
    "\n",
    "    elif config['use_drive']:\n",
    "        print(\"load Dataset From Drive: \")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "        # config['dataset_dir'] = '/content/drive/MyDrive/Arabic_English_OCR_Dataset'\n",
    "        config['dataset_dir'] = '/content/drive/MyDrive/output'\n",
    "    else:\n",
    "        print(\"Dataset already exists, skipping download.\")\n",
    "else:\n",
    "    print(\"====================== Load From Local ======================\")\n",
    "    # config['dataset_dir'] = '/kaggle/input/arabic-english-ocr-dataset/Arabic_English_OCR_Dataset'\n",
    "    config['dataset_dir'] = '/kaggle/input/arabic-english-ocr-synthatic-dataset-v2/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bs0LJHljz0q"
   },
   "source": [
    "## Load Text (labels) & Paths of Images & Visualize sample Before Preprocessig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:04.743045Z",
     "iopub.status.busy": "2025-09-10T13:40:04.742796Z",
     "iopub.status.idle": "2025-09-10T13:40:06.769101Z",
     "shell.execute_reply": "2025-09-10T13:40:06.768197Z",
     "shell.execute_reply.started": "2025-09-10T13:40:04.743024Z"
    },
    "id": "rwNUKBztZg0M",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ar_dir_path = os.path.join(config['dataset_dir'], config['ar_dir'])\n",
    "en_dir_path = os.path.join(config['dataset_dir'], config['en_dir'])\n",
    "mixed_dir_path = os.path.join(config['dataset_dir'], config['mixed_dir'])\n",
    "\n",
    "ar_labels = []\n",
    "ar_labels_file_path = os.path.join(ar_dir_path, config['labels_file'])\n",
    "if os.path.exists(ar_labels_file_path):\n",
    "    with open(ar_labels_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                image_name, text = line.strip().split(' ', 1)\n",
    "                ar_labels.append((os.path.join(ar_dir_path, image_name), text))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping malformed line in {ar_labels_file_path}: {line.strip()}\")\n",
    "else:\n",
    "    print(f\"Warning: Arabic labels file not found at {ar_labels_file_path}\")\n",
    "\n",
    "\n",
    "en_labels = []\n",
    "en_labels_file_path = os.path.join(en_dir_path, config['labels_file'])\n",
    "if os.path.exists(en_labels_file_path):\n",
    "    with open(en_labels_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                image_name, text = line.strip().split(' ', 1)\n",
    "                en_labels.append((os.path.join(en_dir_path, image_name), text))\n",
    "            except ValueError:\n",
    "                 print(f\"Skipping malformed line in {en_labels_file_path}: {line.strip()}\")\n",
    "else:\n",
    "    print(f\"Warning: English labels file not found at {en_labels_file_path}\")\n",
    "\n",
    "\n",
    "mixed_labels = []\n",
    "mixed_labels_file_path = os.path.join(mixed_dir_path, config['labels_file'])\n",
    "if os.path.exists(mixed_labels_file_path):\n",
    "    with open(mixed_labels_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "             try:\n",
    "                image_name, text = line.strip().split(' ', 1)\n",
    "                mixed_labels.append((os.path.join(mixed_dir_path, image_name), text))\n",
    "             except ValueError:\n",
    "                 print(f\"Skipping malformed line in {mixed_labels_file_path}: {line.strip()}\")\n",
    "else:\n",
    "    print(f\"Warning: Mixed labels file not found at {mixed_labels_file_path}\")\n",
    "\n",
    "\n",
    "print(\"After loading: \")\n",
    "print(f\"Total number of examples in ar: {len(ar_labels)}\")\n",
    "print(f\"Total number of examples in en: {len(en_labels)}\")\n",
    "print(f\"Total number of examples in mixed: {len(mixed_labels)}\")\n",
    "\n",
    "\n",
    "# Filter labels to only include permissible characters\n",
    "def filter_lebels(text):\n",
    "    return ''.join(c for c in text if c in config['permissible_chars'])\n",
    "\n",
    "def is_permissible_ar(text):\n",
    "    return all(c in config['permissible_chars'] for c in text)\n",
    "\n",
    "\n",
    "def is_permissible_en(text):\n",
    "    # Define Arabic digits and characters\n",
    "    arabic_digits = set(\"٠١٢٣٤٥٦٧٨٩\")\n",
    "    arabic_chars = set(\"ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيًٌٍَُِّْ\")\n",
    "    # Check if all characters are permissible AND none of them are Arabic digits or characters\n",
    "    return all(c in config['permissible_chars'] for c in text) and not any(c in arabic_digits or c in arabic_chars for c in text)\n",
    "\n",
    "def is_permissible_mixed(text):\n",
    "    return is_permissible_ar(text) or is_permissible_en(text)\n",
    "\n",
    "if config['remove_bad_examples']:\n",
    "    ar_labels = [(name, text) for name, text in ar_labels if is_permissible_ar(text)]\n",
    "    en_labels = [(name, text) for name, text in en_labels if is_permissible_en(text)]\n",
    "    mixed_labels = [(name, text) for name, text in mixed_labels if is_permissible_mixed(text)]\n",
    "else:\n",
    "    ar_labels = [(name, filter_lebels(text)) for name, text in ar_labels]\n",
    "    en_labels = [(name, filter_lebels(text)) for name, text in en_labels]\n",
    "    mixed_labels = [(name, filter_lebels(text)) for name, text in mixed_labels]\n",
    "\n",
    "\n",
    "print(\"After filtering: \")\n",
    "print(f\"Total number of examples in ar: {len(ar_labels)}\")\n",
    "print(f\"Total number of examples in en: {len(en_labels)}\")\n",
    "print(f\"Total number of examples in mixed: {len(mixed_labels)}\")\n",
    "\n",
    "\n",
    "# Take the chosen sample size from each category\n",
    "# Distribute max_samples equally among ar, en, and mixed\n",
    "samples_per_category = config['max_samples'] // 3\n",
    "\n",
    "ar_labels = ar_labels[:min(samples_per_category, len(ar_labels))]\n",
    "en_labels = en_labels[:min(samples_per_category, len(en_labels))]\n",
    "mixed_labels = mixed_labels[:min(samples_per_category, len(mixed_labels))]\n",
    "\n",
    "print(\"After taking the choosen sample size: \")\n",
    "print(f\"Total number of examples in ar: {len(ar_labels)}\")\n",
    "print(f\"Total number of examples in en: {len(en_labels)}\")\n",
    "print(f\"Total number of examples in mixed: {len(mixed_labels)}\")\n",
    "\n",
    "\n",
    "# Visualize a sample image and its label\n",
    "def visualize(labels, lang='en', n=4):\n",
    "    if labels:\n",
    "        # Added check to ensure there are enough samples to visualize\n",
    "        num_samples_to_viz = min(n, len(labels))\n",
    "        sample = labels[:num_samples_to_viz]\n",
    "\n",
    "        # Determine figure title based on language/type\n",
    "        if lang == 'ar':\n",
    "            fig_title = \"Sample Arabic Images\"\n",
    "        elif lang == 'en':\n",
    "            fig_title = \"Sample English Images\"\n",
    "        elif lang == 'mixed':\n",
    "             fig_title = \"Sample Mixed Images\"\n",
    "        else:\n",
    "             fig_title = \"Sample Images\"\n",
    "\n",
    "        # Determine grid size (e.g., 4 columns, rows based on num_samples)\n",
    "        cols = 4\n",
    "        rows = (num_samples_to_viz + cols - 1) // cols\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 5))\n",
    "        axes = axes.ravel() if isinstance(axes, np.ndarray) else [axes] # Handle single subplot case\n",
    "\n",
    "\n",
    "        print(f\"====================== {fig_title} ======================\")\n",
    "        for i, (sample_image_path, sample_label) in enumerate(sample):\n",
    "            try:\n",
    "                img = cv2.imread(sample_image_path)\n",
    "                if img is not None:\n",
    "                    # Convert BGR to RGB for displaying\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    if lang == 'ar' or lang == 'mixed':\n",
    "                        # Reshape and reorder Arabic text for correct display\n",
    "                        # Check if the text contains Arabic characters before reshaping/reordering\n",
    "                        arabic_chars_set = set(\"ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيًٌٍَُِّْ\")\n",
    "                        if any(c in arabic_chars_set for c in sample_label):\n",
    "                            reshaped_text = arabic_reshaper.reshape(sample_label)\n",
    "                            display_label = get_display(reshaped_text)\n",
    "                        else:\n",
    "                            # If no Arabic characters, display as is (might be numbers/symbols/English in mixed)\n",
    "                            display_label = sample_label\n",
    "                    else: # English\n",
    "                        display_label = sample_label\n",
    "\n",
    "                    axes[i].imshow(img_rgb)\n",
    "                    axes[i].set_title(f\"Label: {display_label}\", fontsize=10)\n",
    "                    axes[i].axis('off') # Hide axes\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error loading image: {sample_image_path}\")\n",
    "                    # Hide subplot if image fails to load\n",
    "                    axes[i].axis('off')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while loading or displaying the image: {e}\")\n",
    "                 # Hide subplot if error occurs\n",
    "                axes[i].axis('off')\n",
    "\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for j in range(num_samples_to_viz, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No examples available for {lang} to visualize.\")\n",
    "\n",
    "\n",
    "print(\"====================== Arabic ======================\")\n",
    "visualize(ar_labels, lang='ar')\n",
    "print(\"====================== English ======================\")\n",
    "visualize(en_labels, lang='en')\n",
    "print(\"====================== Mixed ======================\")\n",
    "visualize(mixed_labels, lang='mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGBAeb_ukI6e"
   },
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.770513Z",
     "iopub.status.busy": "2025-09-10T13:40:06.770180Z",
     "iopub.status.idle": "2025-09-10T13:40:06.777293Z",
     "shell.execute_reply": "2025-09-10T13:40:06.776405Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.770489Z"
    },
    "id": "KLhfzrA0rO55",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Unified character set\n",
    "if config['do_analysis']:\n",
    "    # Calculate the unified character set\n",
    "    all_chars = set()\n",
    "    for _, text in ar_labels:\n",
    "        all_chars.update(text)\n",
    "    for _, text in en_labels:\n",
    "        all_chars.update(text)\n",
    "    for _, text in mixed_labels:\n",
    "        all_chars.update(text)\n",
    "\n",
    "    unified_charset = sorted(list(all_chars))\n",
    "    config['vocab_size'] = len(unified_charset) # This will be updated by TextProcessor\n",
    "\n",
    "    print(f\"Unified Character Set (Length): {len(unified_charset)}\")\n",
    "    print(\"Unified Character Set (Characters):\", \"\".join(unified_charset)) # Commented out as it can be very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.778827Z",
     "iopub.status.busy": "2025-09-10T13:40:06.778450Z",
     "iopub.status.idle": "2025-09-10T13:40:06.799081Z",
     "shell.execute_reply": "2025-09-10T13:40:06.798207Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.778803Z"
    },
    "id": "1Y7Z6Uf4u0ea",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config['do_analysis']:\n",
    "    # Analyze filtered dataset - Text length statistics\n",
    "    ar_text_lengths_filtered = [len(text) for _, text in ar_labels]\n",
    "    en_text_lengths_filtered = [len(text) for _, text in en_labels]\n",
    "    mixed_text_lengths_filtered = [len(text) for _, text in mixed_labels]\n",
    "\n",
    "    print(\"\\nFiltered Arabic Text Lengths:\")\n",
    "    print(f\"  Max: {max(ar_text_lengths_filtered) if ar_text_lengths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean: {np.mean(ar_text_lengths_filtered):.2f}\")\n",
    "    print(f\"  Min: {min(ar_text_lengths_filtered) if ar_text_lengths_filtered else 'N/A'}\")\n",
    "\n",
    "    print(\"\\nFiltered English Text Lengths:\")\n",
    "    print(f\"  Max: {max(en_text_lengths_filtered) if en_text_lengths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean: {np.mean(en_text_lengths_filtered):.2f}\")\n",
    "    print(f\"  Min: {min(en_text_lengths_filtered) if en_text_lengths_filtered else 'N/A'}\")\n",
    "\n",
    "    # New: Print mixed text lengths\n",
    "    print(\"\\nFiltered Mixed Text Lengths:\")\n",
    "    print(f\"  Max: {max(mixed_text_lengths_filtered) if mixed_text_lengths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean: {np.mean(mixed_text_lengths_filtered):.2f}\")\n",
    "    print(f\"  Min: {min(mixed_text_lengths_filtered) if mixed_text_lengths_filtered else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.800144Z",
     "iopub.status.busy": "2025-09-10T13:40:06.799907Z",
     "iopub.status.idle": "2025-09-10T13:40:06.817155Z",
     "shell.execute_reply": "2025-09-10T13:40:06.816409Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.800124Z"
    },
    "id": "ce1632a3",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config['do_analysis']:\n",
    "    # Analyze filtered dataset - Image shape statistics\n",
    "    ar_image_shapes_filtered = []\n",
    "    for image_path, _ in ar_labels:\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                ar_image_shapes_filtered.append(img.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image for shape analysis: {image_path} - {e}\")\n",
    "\n",
    "    en_image_shapes_filtered = []\n",
    "    for image_path, _ in en_labels:\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                en_image_shapes_filtered.append(img.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image for shape analysis: {image_path} - {e}\")\n",
    "\n",
    "    mixed_image_shapes_filtered = []\n",
    "    for image_path, _ in mixed_labels:\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                mixed_image_shapes_filtered.append(img.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image for shape analysis: {image_path} - {e}\")\n",
    "\n",
    "\n",
    "    # Assuming all images are color (height, width, channels)\n",
    "    ar_heights_filtered = [shape[0] for shape in ar_image_shapes_filtered]\n",
    "    ar_widths_filtered = [shape[1] for shape in ar_image_shapes_filtered]\n",
    "    en_heights_filtered = [shape[0] for shape in en_image_shapes_filtered]\n",
    "    en_widths_filtered = [shape[1] for shape in en_image_shapes_filtered]\n",
    "    mixed_heights_filtered = [shape[0] for shape in mixed_image_shapes_filtered]\n",
    "    mixed_widths_filtered = [shape[1] for shape in mixed_image_shapes_filtered]\n",
    "\n",
    "\n",
    "    print(\"\\nFiltered Arabic Image Shapes (Height, Width):\")\n",
    "    print(f\"  Max Height: {max(ar_heights_filtered) if ar_heights_filtered else 'N/A'}, Max Width: {max(ar_widths_filtered) if ar_widths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean Height: {np.mean(ar_heights_filtered):.2f} , Mean Width: {np.mean(ar_widths_filtered):.2f}\")\n",
    "    print(f\"  Min Height: {min(ar_heights_filtered) if ar_heights_filtered else 'N/A'}, Min Width: {min(ar_widths_filtered) if ar_widths_filtered else 'N/A'}\")\n",
    "\n",
    "    print(\"\\nFiltered English Image Shapes (Height, Width):\")\n",
    "    print(f\"  Max Height: {max(en_heights_filtered) if en_heights_filtered else 'N/A'}, Max Width: {max(en_widths_filtered) if en_widths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean Height: {np.mean(en_heights_filtered):.2f}, Mean Width: {np.mean(en_widths_filtered):.2f}\")\n",
    "    print(f\"  Min Height: {min(en_heights_filtered) if en_heights_filtered else 'N/A'}, Min Width: {min(en_widths_filtered) if en_widths_filtered else 'N/A'}\")\n",
    "\n",
    "    # New: Print mixed image shapes\n",
    "    print(\"\\nFiltered Mixed Image Shapes (Height, Width):\")\n",
    "    print(f\"  Max Height: {max(mixed_heights_filtered) if mixed_heights_filtered else 'N/A'}, Max Width: {max(mixed_widths_filtered) if mixed_widths_filtered else 'N/A'}\")\n",
    "    print(f\"  Mean Height: {np.mean(mixed_heights_filtered):.2f}, Mean Width: {np.mean(mixed_widths_filtered):.2f}\")\n",
    "    print(f\"  Min Height: {min(mixed_heights_filtered) if mixed_heights_filtered else 'N/A'}, Min Width: {min(mixed_widths_filtered) if mixed_widths_filtered else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-u-JDfs2NVi"
   },
   "source": [
    "# SECTION 4: TEXT PREPROCESSING AND VOCABULARY CREATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.818412Z",
     "iopub.status.busy": "2025-09-10T13:40:06.818116Z",
     "iopub.status.idle": "2025-09-10T13:40:06.838607Z",
     "shell.execute_reply": "2025-09-10T13:40:06.837787Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.818388Z"
    },
    "id": "Vh-8Y75lz_g-",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, unified_charset, max_length=64):\n",
    "        self.max_length = max_length\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.START_TOKEN = '<START>'\n",
    "        self.END_TOKEN = '<END>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.BLANK_TOKEN = '<BLANK>'\n",
    "\n",
    "        # Build vocabulary using the provided unified_charset\n",
    "        vocab = [self.PAD_TOKEN, self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN, self.BLANK_TOKEN] + sorted(list(unified_charset))\n",
    "\n",
    "        # Create mappings\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        # Print sample characters and special tokens for verification\n",
    "        print(f\"Characters: {sorted(list(unified_charset))[:]}\") # Print only first 10 chars\n",
    "        print(f\"Special tokens and their indices: PAD={self.char2idx[self.PAD_TOKEN]}, START={self.char2idx[self.START_TOKEN]}, END={self.char2idx[self.END_TOKEN]}, UNK={self.char2idx[self.UNK_TOKEN]}, BLANK={self.char2idx[self.BLANK_TOKEN]}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Convert text to sequence of indices and pad/truncate\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        # For CTC targets, we only need the characters + UNK and then pad with PAD.\n",
    "        sequence = [self.char2idx.get(char, self.char2idx.get(self.UNK_TOKEN)) for char in text]\n",
    "\n",
    "        # Truncate or pad to max_length\n",
    "        if len(sequence) > self.max_length:\n",
    "             sequence = sequence[:self.max_length]\n",
    "        elif len(sequence) < self.max_length:\n",
    "            sequence.extend([self.char2idx.get(self.PAD_TOKEN)] * (self.max_length - len(sequence)))\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def decode_sequence(self, sequence):\n",
    "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
    "        text = \"\"\n",
    "        for idx in sequence:\n",
    "            char = self.idx2char.get(idx, self.UNK_TOKEN)\n",
    "            # Exclude PAD, START, END, UNK, and BLANK tokens from the decoded text\n",
    "            if char not in [self.PAD_TOKEN, self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN, self.BLANK_TOKEN]:\n",
    "                text += char\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.841906Z",
     "iopub.status.busy": "2025-09-10T13:40:06.841436Z",
     "iopub.status.idle": "2025-09-10T13:40:06.954892Z",
     "shell.execute_reply": "2025-09-10T13:40:06.954081Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.841886Z"
    },
    "id": "f6814446",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the TextProcessor\n",
    "all_chars_for_processor = set()\n",
    "for _, text in ar_labels:\n",
    "    all_chars_for_processor.update(text)\n",
    "for _, text in en_labels:\n",
    "    all_chars_for_processor.update(text)\n",
    "for _, text in mixed_labels:\n",
    "     all_chars_for_processor.update(text)\n",
    "\n",
    "unified_charset_for_processor = sorted(list(all_chars_for_processor))\n",
    "\n",
    "\n",
    "text_processor = TextProcessor(unified_charset_for_processor, max_length=config['max_text_length'])\n",
    "config['vocab_size'] = text_processor.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhEnDOXL3y1L"
   },
   "source": [
    "# SECTION 5: DATASET CLASS AND DATALOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwIwYIl931QI"
   },
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.956025Z",
     "iopub.status.busy": "2025-09-10T13:40:06.955716Z",
     "iopub.status.idle": "2025-09-10T13:40:06.963462Z",
     "shell.execute_reply": "2025-09-10T13:40:06.962485Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.955998Z"
    },
    "id": "BwKMDkTm3q3S",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, labels, text_processor, transform=None):\n",
    "        self.labels = labels\n",
    "        self.text_processor = text_processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text = self.labels[idx]\n",
    "\n",
    "        # Process image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # Handle cases where image loading fails, e.g., return None or a placeholder\n",
    "            print(f\"Warning: Could not load image {image_path}. Skipping.\")\n",
    "            return None # Or handle as appropriate\n",
    "\n",
    "        # Convert to RGB if needed\n",
    "        if len(image.shape) == 3 and image.shape[2] == 4:  # RGBA\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        elif len(image.shape) == 2:  # Grayscale\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # Process text\n",
    "        encoded_text = self.text_processor.encode_text(text)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'encoded_text': torch.tensor(encoded_text, dtype=torch.long),\n",
    "            'text_length': len(text)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUZnxuST6LE5"
   },
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.964858Z",
     "iopub.status.busy": "2025-09-10T13:40:06.964517Z",
     "iopub.status.idle": "2025-09-10T13:40:06.985270Z",
     "shell.execute_reply": "2025-09-10T13:40:06.984417Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.964837Z"
    },
    "id": "5MlgR3Wd6MGO",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_train_transforms(dataset_mean, dataset_std, augmentation_strength=0.25):\n",
    "    \"\"\"\n",
    "    Returns augmentation transforms for training based on augmentation_strength (0 to 1).\n",
    "    augmentation_strength = 0 -> no augmentation (similar to validation transforms)\n",
    "    augmentation_strength = 1 -> maximum configured augmentation\n",
    "    \"\"\"\n",
    "    # Define maximum probabilities and limits for augmentation_strength = 1.0\n",
    "    max_aug_prob = 0.7 # Max overall augmentation probability\n",
    "    max_rotation_limit = 10 # Increased limit for stronger rotation at max strength\n",
    "    max_blur_limit = 5 # Increased limit for stronger blur at max strength\n",
    "    max_brightness_limit = 0.3 # Increased limit\n",
    "    max_contrast_limit = 0.3 # Increased limit\n",
    "    max_distortion_limit = 0.5 # Increased limit\n",
    "    max_elastic_alpha = 10\n",
    "    max_elastic_sigma = 50\n",
    "\n",
    "\n",
    "    # Scale probabilities and limits linearly based on the augmentation strength\n",
    "    aug_prob = max_aug_prob * augmentation_strength\n",
    "    rotation_limit = max_rotation_limit * augmentation_strength\n",
    "    blur_limit = int(max_blur_limit * augmentation_strength)\n",
    "    brightness_limit = max_brightness_limit * augmentation_strength\n",
    "    contrast_limit = max_contrast_limit * augmentation_strength\n",
    "    distortion_limit = max_distortion_limit * augmentation_strength\n",
    "    elastic_alpha = max_elastic_alpha * augmentation_strength\n",
    "    elastic_sigma = max_elastic_sigma * augmentation_strength\n",
    "\n",
    "\n",
    "    # Ensure blur_limit is at least 1 if greater than 0 after scaling\n",
    "    blur_limit = max(1, blur_limit) if blur_limit > 0 else 0\n",
    "\n",
    "\n",
    "    return A.Compose([\n",
    "        A.Resize(config['img_height'], config['img_width']),\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=blur_limit, p=0.5 * augmentation_strength), # Scale internal probabilities as well\n",
    "            A.MotionBlur(blur_limit=blur_limit, p=0.5 * augmentation_strength),\n",
    "        ], p=aug_prob),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=brightness_limit,\n",
    "            contrast_limit=contrast_limit,\n",
    "            p=aug_prob\n",
    "        ),\n",
    "        # Use linear interpolation and reflect border mode for transformations\n",
    "        A.Rotate(limit=rotation_limit, p=aug_prob * 0.6, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101), # Rotate prob can be slightly lower\n",
    "        A.OneOf([\n",
    "            A.GridDistortion(num_steps=5, distort_limit=distortion_limit, p=0.5 * augmentation_strength, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101),\n",
    "            A.ElasticTransform(alpha=elastic_alpha, sigma=elastic_sigma, p=0.8 * augmentation_strength, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101),\n",
    "        ], p=aug_prob),\n",
    "        A.Normalize(mean=dataset_mean, std=dataset_std),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(dataset_mean, dataset_std, augmentation_strength=0.0):\n",
    "    \"\"\"\n",
    "    Returns augmentation transforms for validation (typically no augmentation).\n",
    "    augmentation_strength = 0 -> no augmentation\n",
    "    \"\"\"\n",
    "    # For validation, strength 0 means no augmentations beyond resizing and normalization.\n",
    "    # We still pass the strength parameter for consistency, but it effectively disables augs.\n",
    "    return A.Compose([\n",
    "        A.Resize(config['img_height'], config['img_width']),\n",
    "        # No other augmentations are applied when strength is 0\n",
    "        A.Normalize(mean=dataset_mean, std=dataset_std),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0stMa2F65BZ"
   },
   "source": [
    "## Get Mean & STD for Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:06.986611Z",
     "iopub.status.busy": "2025-09-10T13:40:06.986273Z",
     "iopub.status.idle": "2025-09-10T13:40:07.007423Z",
     "shell.execute_reply": "2025-09-10T13:40:07.006565Z",
     "shell.execute_reply.started": "2025-09-10T13:40:06.986572Z"
    },
    "id": "VeTFA-sC68qm",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_mean_std(labels):\n",
    "    # Initialize variables for sum of pixel values and total pixel count\n",
    "    sum_pixels = np.zeros(3, dtype=np.float64)\n",
    "    total_pixels = 0\n",
    "\n",
    "    # Iterate through the labels to calculate the sum of pixel values\n",
    "    for image_path, _ in tqdm(labels, desc=\"Summing pixels\"):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not load image {image_path}. Skipping for mean/std calculation.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to RGB if needed\n",
    "            if len(image.shape) == 3 and image.shape[2] == 4:  # RGBA\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "            elif len(image.shape) == 2:  # Grayscale\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # Ensure image is float64 for accurate summation\n",
    "            image = image.astype(np.float64)\n",
    "\n",
    "            sum_pixels += np.sum(image, axis=(0, 1))\n",
    "            total_pixels += image.shape[0] * image.shape[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path} for mean/std calculation: {e}\")\n",
    "\n",
    "\n",
    "    # Calculate the mean for each channel\n",
    "    mean_pixels = sum_pixels / total_pixels if total_pixels > 0 else np.zeros(3)\n",
    "\n",
    "    # Initialize variables for sum of squared differences\n",
    "    sum_sq_diff = np.zeros(3, dtype=np.float64)\n",
    "\n",
    "    # Iterate through the labels again to calculate the sum of squared differences\n",
    "    for image_path, _ in tqdm(labels, desc=\"Summing squared diffs\"):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue # Already warned and skipped in the first pass\n",
    "\n",
    "            # Convert to RGB if needed\n",
    "            if len(image.shape) == 3 and image.shape[2] == 4:  # RGBA\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "            elif len(image.shape) == 2:  # Grayscale\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # Ensure image is float64 for accurate calculation\n",
    "            image = image.astype(np.float64)\n",
    "\n",
    "            # Calculate the squared difference from the mean\n",
    "            sq_diff = (image - mean_pixels)**2\n",
    "            sum_sq_diff += np.sum(sq_diff, axis=(0, 1))\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing image {image_path} for mean/std calculation (squared diffs): {e}\")\n",
    "\n",
    "\n",
    "    # Calculate the variance and standard deviation for each channel\n",
    "    variance_pixels = sum_sq_diff / total_pixels if total_pixels > 0 else np.zeros(3)\n",
    "    std_pixels = np.sqrt(variance_pixels)\n",
    "\n",
    "    mean_pixels = mean_pixels / 255.0\n",
    "    std_pixels = std_pixels / 255.0\n",
    "\n",
    "    return mean_pixels.tolist(), std_pixels.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7ix3d9P33vH"
   },
   "source": [
    "## Dataloader & Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:07.008641Z",
     "iopub.status.busy": "2025-09-10T13:40:07.008295Z",
     "iopub.status.idle": "2025-09-10T13:40:07.129259Z",
     "shell.execute_reply": "2025-09-10T13:40:07.128470Z",
     "shell.execute_reply.started": "2025-09-10T13:40:07.008618Z"
    },
    "id": "ceb4c25d",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine Arabic, English, and Mixed labels\n",
    "all_labels = ar_labels + en_labels + mixed_labels\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "random.shuffle(all_labels)\n",
    "\n",
    "\n",
    "# Calculate sizes for train, val, and test\n",
    "total_size = len(all_labels)\n",
    "# Use the split percentages from the config\n",
    "train_size = int(config['train_split_percentage'] * total_size)\n",
    "val_size = int(config['val_split_percentage'] * total_size)\n",
    "test_size = total_size - train_size - val_size # Ensure all samples are included\n",
    "\n",
    "\n",
    "# First split: train and temp (val + test)\n",
    "train_labels, temp_labels = train_test_split(\n",
    "    all_labels,\n",
    "    train_size=train_size,\n",
    "    random_state=config['SEED'] # Use a fixed random state for reproducibility\n",
    ")\n",
    "\n",
    "# Second split: val and test from temp\n",
    "val_labels, test_labels = train_test_split(\n",
    "    temp_labels,\n",
    "    train_size=(val_size / (val_size + test_size) if (val_size + test_size) > 0 else 0), # Ensure non-zero division\n",
    "    random_state=config['SEED'] # Use a fixed random state for reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Total number of samples: {len(all_labels)}\")\n",
    "print(f\"Training set size: {len(train_labels)}\")\n",
    "print(f\"Validation set size: {len(val_labels)}\")\n",
    "print(f\"Test set size: {len(test_labels)}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None values from the batch (in case image loading failed in the dataset)\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None # Return None if batch is empty after filtering\n",
    "\n",
    "    # Use pad_sequence for images if they have variable widths/heights\n",
    "    # However, the current PatchEmbed and SVTR architecture expects fixed input size,\n",
    "    # so Resize in transforms should handle this. We can keep stack if Resize is applied.\n",
    "    # If using variable size input later, this would need pad_sequence or custom padding.\n",
    "    try:\n",
    "      images = torch.stack([item['image'] for item in batch])\n",
    "    except RuntimeError as e:\n",
    "        if \"stack expects each tensor to be of the same size\" in str(e):\n",
    "            print(\"Error: Images in the batch have inconsistent sizes after transformations.\")\n",
    "            print(\"This might be due to incorrect resizing or padding in the dataset or transforms.\")\n",
    "            # You might want to inspect batch[0]['image'].shape etc. for debugging\n",
    "            raise e # Re-raise the error after printing info\n",
    "        else:\n",
    "             raise e\n",
    "\n",
    "\n",
    "    texts = [item['text'] for item in batch]\n",
    "    # encoded_texts are now padded by TextProcessor encode_text, so we can stack them.\n",
    "    # Ensure encoded_texts are consistent length (config['max_text_length'])\n",
    "    try:\n",
    "        encoded_texts = torch.stack([item['encoded_text'] for item in batch]) # Stack already padded tensors\n",
    "    except RuntimeError as e:\n",
    "         if \"stack expects each tensor to be of the same size\" in str(e):\n",
    "              print(\"Error: Encoded texts in the batch have inconsistent sizes after TextProcessor.\")\n",
    "              print(\"This should not happen if max_text_length is applied correctly in encode_text.\")\n",
    "              # You might want to inspect [item['encoded_text'].shape for item in batch] for debugging\n",
    "              raise e # Re-raise the error after printing info\n",
    "         else:\n",
    "              raise e\n",
    "\n",
    "\n",
    "    # text_lengths from item['text_length'] should be the ORIGINAL lengths, not padded length\n",
    "    text_lengths = torch.tensor([item['text_length'] for item in batch], dtype=torch.long) # Use original text lengths\n",
    "\n",
    "\n",
    "    return {\n",
    "        'images': images,\n",
    "        'texts': texts,\n",
    "        'encoded_texts': encoded_texts, # Use padded sequences from TextProcessor\n",
    "        'text_lengths': text_lengths # Use original text lengths\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate mean and std of the dataset for normalization\n",
    "# Only calculate if dataset_mean is not already provided in config\n",
    "if config['dataset_mean'] is None or config['dataset_std'] is None:\n",
    "    print(\"\\nCalculating dataset mean and standard deviation for normalization...\")\n",
    "    # Use the full combined dataset for calculation\n",
    "    dataset_mean, dataset_std = get_mean_std(all_labels)\n",
    "    config['dataset_mean'] = dataset_mean # Update config with calculated values\n",
    "    config['dataset_std'] = dataset_std\n",
    "    print(\"Calculation complete.\")\n",
    "    print(f\"Dataset Mean: {dataset_mean}\")\n",
    "    print(f\"Dataset Std: {dataset_std}\")\n",
    "else:\n",
    "    dataset_mean = config['dataset_mean']\n",
    "    dataset_std = config['dataset_std']\n",
    "    print(f\"\\nUsing pre-configured Dataset Mean: {dataset_mean}\")\n",
    "    print(f\"Using pre-configured Dataset Std: {dataset_std}\")\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OCRDataset(\n",
    "    train_labels,\n",
    "    text_processor,\n",
    "    transform=get_train_transforms(config['dataset_mean'], config['dataset_std'], config['augmentation_strength'])\n",
    ")\n",
    "\n",
    "val_dataset = OCRDataset(\n",
    "    val_labels,\n",
    "    text_processor,\n",
    "    transform=get_val_transforms(config['dataset_mean'], config['dataset_std'], 0.0) # Validation uses strength 0\n",
    ")\n",
    "\n",
    "test_dataset = OCRDataset(\n",
    "    test_labels,\n",
    "    text_processor,\n",
    "    transform=get_val_transforms(config['dataset_mean'], config['dataset_std'], 0.0) # Test uses strength 0\n",
    ")\n",
    "\n",
    "# DataLoader parameters from config\n",
    "dataloader_params = config['dataloader_params']\n",
    "\n",
    "# Create dataloaders\n",
    "# Add error handling for DataLoader creation if num_workers causes issues\n",
    "try:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        **dataloader_params\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        **dataloader_params\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        **dataloader_params\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "     if \"DataLoader worker (pid(s)\" in str(e) and \"KeyboardInterrupt\" not in str(e):\n",
    "         print(\"\\nError: DataLoader failed, possibly due to num_workers issues in this environment.\")\n",
    "         print(\"Trying again with num_workers=0...\")\n",
    "         # Retry with num_workers=0\n",
    "         dataloader_params_no_workers = config['dataloader_params'].copy()\n",
    "         dataloader_params_no_workers['num_workers'] = 0\n",
    "         if 'pin_memory' in dataloader_params_no_workers:\n",
    "             dataloader_params_no_workers['pin_memory'] = False\n",
    "         if 'persistent_workers' in dataloader_params_no_workers:\n",
    "              dataloader_params_no_workers['persistent_workers'] = False\n",
    "\n",
    "         train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            **dataloader_params_no_workers\n",
    "        )\n",
    "\n",
    "         val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            **dataloader_params_no_workers\n",
    "        )\n",
    "\n",
    "         test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            **dataloader_params_no_workers\n",
    "        )\n",
    "         print(\"DataLoaders created with num_workers=0.\")\n",
    "     else:\n",
    "         # Re-raise other RuntimeError\n",
    "         raise e\n",
    "\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf4J7HcQ_aRH"
   },
   "source": [
    "# SECTION 6: VISUALIZE SAMPLES FROM DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:07.130491Z",
     "iopub.status.busy": "2025-09-10T13:40:07.130155Z",
     "iopub.status.idle": "2025-09-10T13:40:08.791300Z",
     "shell.execute_reply": "2025-09-10T13:40:08.790236Z",
     "shell.execute_reply.started": "2025-09-10T13:40:07.130468Z"
    },
    "id": "NgLSanrW5hOP",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_batch(dataloader, text_processor, num_samples=8):\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    # Ensure num_samples does not exceed the actual batch size\n",
    "    actual_batch_size = batch['images'].size(0)\n",
    "    num_samples_to_show = min(num_samples, actual_batch_size)\n",
    "\n",
    "    images = batch['images'][:num_samples_to_show]\n",
    "    texts = batch['texts'][:num_samples_to_show]\n",
    "\n",
    "    # Determine grid size (e.g., 4 columns, rows based on num_samples)\n",
    "    cols = 4\n",
    "    rows = (num_samples_to_show + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 5))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Get the dataset mean and std from the config or a global variable if available\n",
    "    # Assuming dataset_mean and dataset_std are available in the global scope or config\n",
    "    # If not, you might need to pass them to this function or retrieve them differently\n",
    "    if 'dataset_mean' in globals() and 'dataset_std' in globals():\n",
    "        mean_val = np.array(dataset_mean)\n",
    "        std_val = np.array(dataset_std)\n",
    "    elif 'dataset_mean' in config and 'dataset_std' in config:\n",
    "         mean_val = np.array(config['dataset_mean'])\n",
    "         std_val = np.array(config['dataset_std'])\n",
    "    else:\n",
    "        # Fallback to default values if not found (though it's better to use calculated values)\n",
    "        print(\"Warning: Dataset mean and std not found. Using default ImageNet values for visualization denormalization.\")\n",
    "        mean_val = np.array([0.485, 0.456, 0.406])\n",
    "        std_val = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    for i in range(num_samples_to_show):\n",
    "        image = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        # Denormalize using the calculated mean and std\n",
    "        image = image * std_val + mean_val\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        # Reshape and get display for Arabic text\n",
    "        reshaped_text = arabic_reshaper.reshape(texts[i])\n",
    "        display_text = get_display(reshaped_text)\n",
    "\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Text: {display_text}\", fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_samples_to_show, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing training samples...\")\n",
    "visualize_batch(train_loader, text_processor, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcVdrbrtLWYS"
   },
   "source": [
    "# SECTION 7: SVTR MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UehGzcWVn2x4"
   },
   "source": [
    "## Utilities: Window Partitioning / Reverse (B[N]CHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:08.794218Z",
     "iopub.status.busy": "2025-09-10T13:40:08.793240Z",
     "iopub.status.idle": "2025-09-10T13:40:08.831251Z",
     "shell.execute_reply": "2025-09-10T13:40:08.830379Z",
     "shell.execute_reply.started": "2025-09-10T13:40:08.794171Z"
    },
    "id": "-8dSvTVtCxy7",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def window_partition_nchw(x, window_size):\n",
    "    \"\"\"\n",
    "    Partition NCHW tensor into windows.\n",
    "    Args:\n",
    "      x: (B, C, H, W)\n",
    "      window_size: (wh, ww)\n",
    "    Returns:\n",
    "      windows: (num_windows_total, wh*ww, C) i.e. (B * n_h, n_w, wh * ww, C) later the num_window_total will be treaded as\n",
    "      normal batches and the attension will work on each window independently, and then recombined via reverse operation.\n",
    "      (Hp, Wp, pad_h, pad_w, n_h, n_w)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    wh, ww = window_size\n",
    "    # Calculate padding size for height and width to be divisible by window size\n",
    "    pad_h = (wh - H % wh) % wh\n",
    "    pad_w = (ww - W % ww) % ww\n",
    "    # Apply padding if needed\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        # F.pad expects (left, right, top, bottom) for the last dimension,\n",
    "        # then the second to last, and so on.\n",
    "        # So for (B, C, H, W), padding is (pad_w_left, pad_w_right, pad_h_top, pad_h_bottom, pad_c_front, pad_c_back, pad_b_front, pad_b_back)\n",
    "        # We only need to pad W and H, so it's (0, pad_w, 0, pad_h) applied to the last two spatial dimensions (W and H)\n",
    "        x = F.pad(x, (0, pad_w, 0, pad_h))\n",
    "    # Calculate padded height and width\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "    # Calculate number of windows along height and width\n",
    "    n_h = Hp // wh\n",
    "    n_w = Wp // ww\n",
    "    # reshape to (B, C, n_h, wh, n_w, ww) - arrange windows within the image\n",
    "    x = x.view(B, C, n_h, wh, n_w, ww)\n",
    "    # permute to (B, n_h, n_w, wh, ww, C) - move channels to the end\n",
    "    x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
    "    # reshape to (num_windows_total, wh*ww, C) - flatten windows and combine batch and window dimensions\n",
    "    windows = x.view(-1, wh * ww, C)  # (B*n_h*n_w, wh*ww, C)\n",
    "    # Return windows and information needed for reverse operation\n",
    "    return windows, (Hp, Wp, pad_h, pad_w, n_h, n_w)\n",
    "\n",
    "def window_reverse_nchw(windows, window_size, Hp, Wp, pad_h, pad_w, n_h, n_w, B):\n",
    "    \"\"\"\n",
    "    Reverse windows back to NCHW tensor.\n",
    "    Args:\n",
    "      windows: (B*n_h*n_w, wh*ww, C) - flattened windows\n",
    "      window_size: (wh, ww) - height and width of each window\n",
    "      Hp, Wp: padded H and W - height and width after padding\n",
    "      pad_h, pad_w: paddings - amount of padding applied to height and width\n",
    "      n_h, n_w: #windows per dim - number of windows along height and width\n",
    "      B: original batch size\n",
    "    Returns:\n",
    "      x: (B, C, H, W) restored (unpadded) tensor\n",
    "    \"\"\"\n",
    "    wh, ww = window_size\n",
    "    C = windows.shape[-1] # Get the number of channels from the windows tensor\n",
    "    # Reshape the windows back to their spatial arrangement within each image\n",
    "    x = windows.view(B, n_h, n_w, wh, ww, C)\n",
    "    # Permute the dimensions back to the original NCHW format (B, C, H, W)\n",
    "    # This reverses the permutation done in window_partition_nchw\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).contiguous()  # (B, C, n_h, wh, n_w, ww)\n",
    "    # Reshape to combine the window dimensions with the number of windows dimensions\n",
    "    x = x.view(B, C, Hp, Wp) # (B, C, Hp, Wp) - padded tensor\n",
    "\n",
    "    # Remove padding if it was applied\n",
    "    if pad_h > 0:\n",
    "        h = Hp - pad_h\n",
    "    else:\n",
    "        h = Hp\n",
    "    if pad_w > 0:\n",
    "        w = Wp - pad_w\n",
    "    else:\n",
    "        w = Wp\n",
    "    # Slice the tensor to remove the padding and get the original height and width\n",
    "    x = x[:, :, :h, :w].contiguous()\n",
    "    return x\n",
    "\n",
    "dummy_tensor_nchw = torch.randn(2, 64, 32, 32) # Example shape\n",
    "window_size_nchw = (7, 7) # Example window size\n",
    "\n",
    "print(f\"Original NCHW tensor shape: {dummy_tensor_nchw.shape}\")\n",
    "\n",
    "# Apply window partition\n",
    "windows_nchw, (Hp_nchw, Wp_nchw, pad_h_nchw, pad_w_nchw, n_h_nchw, n_w_nchw) = window_partition_nchw(dummy_tensor_nchw, window_size_nchw)\n",
    "\n",
    "print(f\"Output windows shape after partition: {windows_nchw.shape}\")\n",
    "print(f\"Padding and window info: Hp={Hp_nchw}, Wp={Wp_nchw}, pad_h={pad_h_nchw}, pad_w={pad_w_nchw}, n_h={n_h_nchw}, n_w={n_w_nchw}\")\n",
    "\n",
    "\n",
    "# Apply window reverse\n",
    "reversed_tensor_nchw = window_reverse_nchw(windows_nchw, window_size_nchw, Hp_nchw, Wp_nchw, pad_h_nchw, pad_w_nchw, n_h_nchw, n_w_nchw, dummy_tensor_nchw.shape[0])\n",
    "\n",
    "print(f\"Output NCHW tensor shape after reverse: {reversed_tensor_nchw.shape}\")\n",
    "\n",
    "# Verify that the reversed tensor is close to the original (before padding)\n",
    "# We need to remove the padding from the original tensor if any was applied\n",
    "original_unpadded = dummy_tensor_nchw[:, :, :dummy_tensor_nchw.shape[2], :dummy_tensor_nchw.shape[3]]\n",
    "\n",
    "# Check if shapes match after reversing and removing padding from original\n",
    "print(f\"Shape of original unpadded tensor: {original_unpadded.shape}\")\n",
    "print(f\"Shape of reversed tensor: {reversed_tensor_nchw.shape}\")\n",
    "\n",
    "# Check if values are close (due to potential floating point inaccuracies)\n",
    "are_tensors_close = torch.allclose(original_unpadded, reversed_tensor_nchw, atol=1e-6)\n",
    "print(f\"Are the original unpadded and reversed tensors close? {are_tensors_close}\")\n",
    "\n",
    "# You can also print the tensors or their difference for detailed inspection if needed\n",
    "print(\"Difference:\", (original_unpadded - reversed_tensor_nchw).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKNYODzyoEby"
   },
   "source": [
    "## Basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:08.832581Z",
     "iopub.status.busy": "2025-09-10T13:40:08.832298Z",
     "iopub.status.idle": "2025-09-10T13:40:08.861828Z",
     "shell.execute_reply": "2025-09-10T13:40:08.860886Z",
     "shell.execute_reply.started": "2025-09-10T13:40:08.832562Z"
    },
    "id": "-d9UFWkMnVXZ",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbedSVTR(nn.Module):\n",
    "    \"\"\"Overlapping Patch Embedding (NCHW) similar to SVTR paper.\"\"\"\n",
    "    def __init__(self, img_size=(64, 256), in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        H, W = img_size\n",
    "        self.img_size = img_size\n",
    "        # two conv layers with stride 2 each -> downsample by (4,4)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim // 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # store patches resolution\n",
    "        self.patches_resolution = (H // 4, W // 4)\n",
    "        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        assert (H, W) == self.img_size, f\"Input size {(H,W)} != expected {self.img_size}\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # now x: (B, embed_dim, H/4, W/4)\n",
    "        B, D, h, w = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, h*w, D)\n",
    "        x = self.norm(x)\n",
    "        return x  # (B, N, D)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, ratio=2.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(in_dim * ratio)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, in_dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.drop(x)\n",
    "\n",
    "# Test the PatchEmbedSVTR\n",
    "patch_embed = PatchEmbedSVTR(img_size=(config['img_height'], config['img_width']), in_chans=config['channels'], embed_dim=64)\n",
    "dummy_input_patch = torch.randn(1, config['channels'], config['img_height'], config['img_width'])\n",
    "output_patch = patch_embed(dummy_input_patch)\n",
    "\n",
    "print(f\"PatchEmbedSVTR Input shape: {dummy_input_patch.shape}\")\n",
    "print(f\"PatchEmbedSVTR Output shape: {output_patch.shape}\")\n",
    "\n",
    "# Test the MLP\n",
    "# The input to MLP should match the output dimension of the PatchEmbed,\n",
    "# which is (B, num_patches, embed_dim)\n",
    "dummy_input_mlp = torch.randn(output_patch.shape) # Using the output shape of PatchEmbed as input\n",
    "mlp = MLP(in_dim=output_patch.shape[-1]) # Initialize MLP with the embedding dimension\n",
    "output_mlp = mlp(dummy_input_mlp)\n",
    "\n",
    "print(f\"\\nMLP Input shape: {dummy_input_mlp.shape}\")\n",
    "print(f\"MLP Output shape: {output_mlp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZGtSKJPob9B"
   },
   "source": [
    "## Global mixing (Transformer-style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:08.863170Z",
     "iopub.status.busy": "2025-09-10T13:40:08.862916Z",
     "iopub.status.idle": "2025-09-10T13:40:09.348256Z",
     "shell.execute_reply": "2025-09-10T13:40:09.347413Z",
     "shell.execute_reply.started": "2025-09-10T13:40:08.863151Z"
    },
    "id": "QUdF1-RYod0B",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GlobalMixing(nn.Module):\n",
    "    \"\"\"Global mixing (Transformer-style) block.\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim) # Layer normalization before attention\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=drop) # Multi-head self-attention\n",
    "        self.norm2 = nn.LayerNorm(dim) # Layer normalization after attention and before MLP\n",
    "        self.mlp = MLP(dim, ratio=mlp_ratio, drop=drop) # MLP block (Feed-forward network)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, D) where B is batch size, N is sequence length, D is dimension\n",
    "        res = x # Residual connection\n",
    "        x = self.norm1(x) # Apply LayerNorm Improved Training Stability\n",
    "        # Apply Multi-head self-attention. Query, Key, and Value are all the same (self-attention).\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = res + attn_out # Add attention output to the residual connection\n",
    "\n",
    "        res = x # Second residual connection\n",
    "        x = self.norm2(x) # Apply LayerNorm\n",
    "        x = res + self.mlp(x) # Add MLP output to the residual connection (including dropout within MLP)\n",
    "        return x # Output tensor with the same shape as input (B, N, D)\n",
    "\n",
    "# Test the GlobalMixing module\n",
    "# The input to GlobalMixing should be in the shape (B, N, D)\n",
    "# where D is the embedding dimension. This could be the output of PatchEmbed or previous blocks.\n",
    "# Using a dummy input similar to the output of PatchEmbed for testing.\n",
    "B, N, D = 2, 2048, 64 # Example Batch size, Sequence length (num_patches), Dimension\n",
    "dummy_input_global = torch.randn(B, N, D)\n",
    "\n",
    "# Instantiate the GlobalMixing module\n",
    "num_heads = 8 # Example number of attention heads\n",
    "global_mixing = GlobalMixing(dim=D, num_heads=num_heads)\n",
    "\n",
    "# Pass the dummy input through the module\n",
    "output_global = global_mixing(dummy_input_global)\n",
    "\n",
    "print(f\"GlobalMixing Input shape: {dummy_input_global.shape}\")\n",
    "print(f\"GlobalMixing Output shape: {output_global.shape}\")\n",
    "\n",
    "# Verify that the output shape is the same as the input shape\n",
    "assert dummy_input_global.shape == output_global.shape, \"Output shape does not match input shape!\"\n",
    "print(\"Test passed: Output shape matches input shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXboeg6-6CG6"
   },
   "source": [
    "## Local mixing using Non-overlapping windows + attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfkGYkOghYAK"
   },
   "source": [
    "### Non-overlapping local attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.349844Z",
     "iopub.status.busy": "2025-09-10T13:40:09.349178Z",
     "iopub.status.idle": "2025-09-10T13:40:09.406612Z",
     "shell.execute_reply": "2025-09-10T13:40:09.405764Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.349815Z"
    },
    "id": "3YSbUG40Xw1k",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Non-Overlapping Local Window Attention mixing\n",
    "class NOLMWA(nn.Module):\n",
    "    \"\"\"\n",
    "    Non-overlapping local window attention:\n",
    "      - partition to windows\n",
    "      - apply MultiheadAttention on each window independently by batching them\n",
    "      - reverse\n",
    "    Inputs to forward(): x is (B, N, D) with h,w passed separately.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2.0, window_size=(7,11), drop=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim # Input dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.wh, self.ww = window_size # Window height and width\n",
    "        self.norm1 = nn.LayerNorm(dim) # Layer normalization before attention\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=drop) # Multi-head self-attention\n",
    "        self.norm2 = nn.LayerNorm(dim) # Layer normalization after attention and before MLP\n",
    "        self.mlp = MLP(dim, ratio=mlp_ratio, drop=drop) # MLP block (Feed-forward network)\n",
    "\n",
    "    def forward(self, x, h, w, shift=(0,0)):\n",
    "        # x: (B, N, D) where B is batch size, N is sequence length (h*w), D is dimension\n",
    "        B, N, D = x.shape\n",
    "        assert N == h * w, \"N must equal h*w\" # Assert that sequence length matches spatial dimensions\n",
    "\n",
    "        res = x # Residual connection\n",
    "\n",
    "        # Pre-normalization\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # to NCHW for windowing - reshape to (B, C, h, w)\n",
    "        x2 = x.transpose(1, 2).reshape(B, D, h, w)\n",
    "\n",
    "        # Partition into windows - windows shape (B*n_h*n_w, wh*ww, C)\n",
    "        windows, (Hp, Wp, pad_h, pad_w, n_h, n_w) = window_partition_nchw(x2, (self.wh, self.ww))\n",
    "\n",
    "        # Apply Multi-head self-attention on windows\n",
    "        # windows shape is (num_windows_total, wh*ww, C) but C==D\n",
    "        attn_out, _ = self.attn(windows, windows, windows)\n",
    "\n",
    "        # Reverse window partitioning - reconstruct to (B, C, h, w)\n",
    "        x2 = window_reverse_nchw(attn_out, (self.wh, self.ww), Hp, Wp, pad_h, pad_w, n_h, n_w, B)\n",
    "\n",
    "        # Reshape back to (B, N, D)\n",
    "        x = x2.view(B, D, h * w).transpose(1, 2).contiguous()\n",
    "\n",
    "        # Add residual connection after attention\n",
    "        x = res + x\n",
    "\n",
    "        # Second residual connection\n",
    "        res = x\n",
    "\n",
    "        # Pre-normalization before MLP\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and add residual connection\n",
    "        x = res + self.mlp(x)\n",
    "\n",
    "        return x # Output tensor with the same shape as input (B, N, D)\n",
    "\n",
    "h, w = 32, 64 # Example spatial dimensions (after patch embedding)\n",
    "dummy_input_local = torch.randn(2, h * w, 64)\n",
    "\n",
    "output_local = NOLMWA(dim=64, num_heads=8, window_size=(7,11))(dummy_input_local, h, w)\n",
    "\n",
    "print(f\"LocalMixingWindow Input shape: {dummy_input_local.shape}\")\n",
    "print(f\"LocalMixingWindow Output shape: {output_local.shape}\")\n",
    "\n",
    "assert dummy_input_local.shape == output_local.shape, \"Output shape does not match input shape!\"\n",
    "print(\"Test passed: Output shape matches input shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc0sTuOXjkS1"
   },
   "source": [
    "### Shifted Window Local Mixing Window Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.408467Z",
     "iopub.status.busy": "2025-09-10T13:40:09.407664Z",
     "iopub.status.idle": "2025-09-10T13:40:09.497311Z",
     "shell.execute_reply": "2025-09-10T13:40:09.496500Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.408435Z"
    },
    "id": "_3nKUGehq_5k",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SWLMWA(nn.Module):\n",
    "    \"\"\"\n",
    "    Local windowed attention with optional cyclic shift and per-window attention mask.\n",
    "    Implements manual multi-head attention so we can apply a (num_windows, ws, ws) mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2.0, window_size=(7,11), drop=0.0, shift_size=(0,0)):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.wh, self.ww = window_size\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.shift_size = shift_size if shift_size is not None else (0,0)\n",
    "\n",
    "        # Layer norm before attention and QKV projection\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        # QKV projection: linearly transform input to Query, Key, and Value\n",
    "        # Output size is 3 * dim for Q, K, and V combined\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        # Output projection after attention\n",
    "        self.proj = nn.Linear(dim, dim, bias=True)\n",
    "\n",
    "        # Layer norm before MLP\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        # MLP (Feed-forward network) - Reusing the MLP class defined earlier\n",
    "        self.mlp = MLP(dim, ratio=mlp_ratio, drop=drop)\n",
    "\n",
    "        # The attention mask will be generated in the forward pass based on input dimensions.\n",
    "        # We can register an empty buffer or None here, or simply generate it directly in forward.\n",
    "        # We will generate it directly in forward.\n",
    "\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        \"\"\"\n",
    "        x: (B, N, D) with N == h*w\n",
    "        h,w: spatial dims (tokens grid) before windowing\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        assert N == h * w, f\"N({N}) must equal h({h})*w({w})\"\n",
    "        device = x.device\n",
    "        res = x # Residual connection\n",
    "\n",
    "        # pre-norm\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # to NCHW for windowing\n",
    "        # Reshape to (B, C, H, W) where C=D, H=h, W=w\n",
    "        x2 = x.transpose(1, 2).reshape(B, D, h, w)  # (B, D, H, W)\n",
    "\n",
    "        # apply cyclic shift if requested (negative roll to shift content BEFORE partitioning,\n",
    "        # which matches Swin's order)\n",
    "        if self.shift_size[0] != 0 or self.shift_size[1] != 0:\n",
    "            shift_h, shift_w = self.shift_size\n",
    "            # Roll the tensor along height and width dimensions\n",
    "            x2 = torch.roll(x2, shifts=(-shift_h, -shift_w), dims=(2, 3))\n",
    "\n",
    "        # partition into windows\n",
    "        # Output windows shape: (B * n_h * n_w, ws, C) where ws = wh*ww\n",
    "        windows, (Hp, Wp, pad_h, pad_w, n_h, n_w) = window_partition_nchw(x2, (self.wh, self.ww))\n",
    "        # windows: (num_windows_total, ws, C) where num_windows_total = B * n_h * n_w\n",
    "\n",
    "        num_windows_total = windows.shape[0] # Total number of windows across the batch\n",
    "        ws = windows.shape[1]  # seq length inside a window\n",
    "        num_windows_per_image = n_h * n_w # Number of windows per image\n",
    "\n",
    "        # Generate attention mask dynamically based on current input dimensions\n",
    "        # Create a mask grid that labels each (Hp x Wp) token with its window index\n",
    "        img_mask = torch.zeros((1, 1, Hp, Wp), device=device, dtype=torch.int32)\n",
    "        cnt = 0\n",
    "        for i in range(0, Hp, self.wh):\n",
    "            for j in range(0, Wp, self.ww):\n",
    "                img_mask[:, :, i:i + self.wh, j:j + self.ww] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        # Partition that mask the same way we partition x2 (but on a single image)\n",
    "        # mask_windows shape: (1 * n_h * n_w, ws, 1)\n",
    "        mask_windows, _ = window_partition_nchw(img_mask, (self.wh, self.ww))\n",
    "\n",
    "        # Reshape mask_windows to (num_windows_per_image, ws)\n",
    "        # Remove the singleton channel dimension from mask_windows\n",
    "        mask_windows = mask_windows.squeeze(-1) # Shape becomes (num_windows_per_image, ws)\n",
    "\n",
    "\n",
    "        # Create per-window boolean mask where True indicates \"forbid attention\"\n",
    "        # (n_h * n_w, ws, ws) of booleans\n",
    "        attn_mask = (mask_windows.unsqueeze(1) != mask_windows.unsqueeze(2))\n",
    "\n",
    "        # Repeat the mask for the batch dimension\n",
    "        # attn_mask shape: (n_h * n_w, ws, ws) -> repeat B times -> (B, n_h * n_w, ws, ws)\n",
    "        repeated_attn_mask = attn_mask.unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "        # Reshape to match the batched windows dimension: (B * n_h * n_w, ws, ws)\n",
    "        repeated_attn_mask = repeated_attn_mask.view(num_windows_total, ws, ws)\n",
    "\n",
    "\n",
    "        # Manual multi-head attention over windows, with mask applied per-window\n",
    "        # windows: (num_windows_total, ws, C)\n",
    "        qkv = self.qkv(windows)  # (num_windows_total, ws, 3*D)\n",
    "        # Split QKV into Query, Key, Value along the last dimension\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # reshape for heads: (num_windows_total, num_heads, ws, head_dim)\n",
    "        def reshape_to_heads(t):\n",
    "            # Reshape to (num_windows_total, ws, num_heads, head_dim)\n",
    "            t = t.view(num_windows_total, ws, self.num_heads, self.head_dim)\n",
    "            # Permute to (num_windows_total, num_heads, ws, head_dim) for batch matrix multiplication\n",
    "            return t.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        q = reshape_to_heads(q)\n",
    "        k = reshape_to_heads(k)\n",
    "        v = reshape_to_heads(v)\n",
    "\n",
    "        # scaled dot-product: (num_windows_total, num_heads, ws, ws)\n",
    "        # Calculate attention scores: Q @ K_transpose / sqrt(head_dim)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # apply mask: set to -inf where forbidden\n",
    "        # Use a large negative number for masked positions so that softmax makes them effectively zero\n",
    "        inf_mask = torch.tensor(-1e9, device=device, dtype=attn.dtype)\n",
    "        attn = attn.masked_fill(repeated_attn_mask.unsqueeze(1), inf_mask)\n",
    "\n",
    "        # softmax & dropout (dropout omitted here but can be added)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        # Optionally add dropout after softmax: attn = F.dropout(attn, p=drop, training=self.training)\n",
    "\n",
    "        # attention output (num_windows_total, num_heads, ws, head_dim)\n",
    "        # Compute the output by multiplying attention probabilities with Value tensor\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # merge heads -> (num_windows_total, ws, D)\n",
    "        # Permute back to (num_windows_total, ws, num_heads, head_dim)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        # Reshape to (num_windows_total, ws, D) by combining num_heads and head_dim\n",
    "        out = out.view(num_windows_total, ws, D)\n",
    "        # Apply output projection\n",
    "        out = self.proj(out)\n",
    "\n",
    "        # reverse windows -> reconstruct (B, C, Hp, Wp)\n",
    "        # Use window_reverse_nchw to bring the windows back to the padded image shape\n",
    "        x2 = window_reverse_nchw(out, (self.wh, self.ww), Hp, Wp, pad_h, pad_w, n_h, n_w, B)\n",
    "\n",
    "        # reverse cyclic shift (if applied) by positive shift\n",
    "        if self.shift_size[0] != 0 or self.shift_size[1] != 0:\n",
    "            shift_h, shift_w = self.shift_size\n",
    "            # Roll back the tensor to reverse the cyclic shift\n",
    "            x2 = torch.roll(x2, shifts=(shift_h, shift_w), dims=(2, 3))\n",
    "\n",
    "        # back to (B, N, D)\n",
    "        # Reshape from (B, D, H, W) to (B, N, D) where N=H*W\n",
    "        x = x2.view(B, D, h * w).transpose(1, 2).contiguous()\n",
    "\n",
    "        # residual + MLP\n",
    "        x = res + x # Add residual connection after attention\n",
    "        res2 = x # Second residual connection before MLP\n",
    "        x = self.norm2(x) # Pre-normalization before MLP\n",
    "        x = res2 + self.mlp(x) # Apply MLP and add residual connection\n",
    "\n",
    "        return x # Output tensor with the same shape as input (B, N, D)\n",
    "\n",
    "h, w = 32, 64 # Example spatial dimensions (after patch embedding)\n",
    "dummy_input_local = torch.randn(2, h * w, D)\n",
    "output_local = SWLMWA(dim=64, num_heads=8, window_size=(7,11), shift_size=(3,5))(dummy_input_local, h, w)\n",
    "\n",
    "print(f\"LocalMixingWindow Input shape: {dummy_input_local.shape}\")\n",
    "print(f\"LocalMixingWindow Output shape: {output_local.shape}\")\n",
    "\n",
    "# Verify that the output shape is the same as the input shape\n",
    "assert dummy_input_local.shape == output_local.shape, \"Output shape does not match input shape!\"\n",
    "print(\"Test passed: Output shape matches input shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVivMR8EkhLw"
   },
   "source": [
    "### Deformable Local Mixing Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.512528Z",
     "iopub.status.busy": "2025-09-10T13:40:09.511691Z",
     "iopub.status.idle": "2025-09-10T13:40:09.531832Z",
     "shell.execute_reply": "2025-09-10T13:40:09.531020Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.512502Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class OffsetPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predict offsets for each query token.\n",
    "\n",
    "    Produces offsets shaped (B, N, H, P, 2) in pixel units (dx, dy).\n",
    "    The output of the linear layer is squashed with tanh and multiplied by offset_scale.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, n_points: int, offset_scale: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_points = n_points\n",
    "        self.offset_scale = float(offset_scale)\n",
    "        self.linear = nn.Linear(dim, num_heads * n_points * 2, bias=True)\n",
    "\n",
    "        # initialize offset predictor to small values so offsets start near zero\n",
    "        nn.init.constant_(self.linear.weight, 0.0)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, N, D)\n",
    "        B, N, D = x.shape\n",
    "        out = self.linear(x)  # (B, N, H * P * 2)\n",
    "        out = out.view(B, N, self.num_heads, self.n_points, 2)  # (B, N, H, P, 2)\n",
    "        out = out.tanh() * self.offset_scale  # scaled pixel offsets (dx, dy)\n",
    "        return out  # (B, N, H, P, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.533085Z",
     "iopub.status.busy": "2025-09-10T13:40:09.532836Z",
     "iopub.status.idle": "2025-09-10T13:40:09.548079Z",
     "shell.execute_reply": "2025-09-10T13:40:09.547019Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.533064Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_base_grid(h: int, w: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Construct base pixel coordinates for each spatial location.\n",
    "    Returned shape: (N, 2) with coordinate order (x, y).\n",
    "    N == h * w\n",
    "    \"\"\"\n",
    "    # coords_y: [0, 1, ..., h-1], coords_x: [0, 1, ..., w-1]\n",
    "    coords_y = torch.arange(h, device=device, dtype=dtype)\n",
    "    coords_x = torch.arange(w, device=device, dtype=dtype)\n",
    "    # meshgrid with indexing='ij' gives grid_y shape (h, w) and grid_x shape (h, w)\n",
    "    grid_y, grid_x = torch.meshgrid(coords_y, coords_x, indexing='ij')\n",
    "    # stack as (x, y) per pixel and flatten to (N, 2)\n",
    "    base_xy = torch.stack((grid_x, grid_y), dim=-1).reshape(-1, 2)  # (N, 2), columns: (x, y)\n",
    "    return base_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.549279Z",
     "iopub.status.busy": "2025-09-10T13:40:09.549050Z",
     "iopub.status.idle": "2025-09-10T13:40:09.568632Z",
     "shell.execute_reply": "2025-09-10T13:40:09.567776Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.549261Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureMapProducer(nn.Module):\n",
    "    \"\"\"\n",
    "    Produce key and value feature maps from flattened tokens (B, N, D).\n",
    "    Produces k_map and v_map each shaped (B, D, h, w).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.kv_conv = nn.Conv2d(dim, dim * 2, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, h: int, w: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # tokens: (B, N, D) where N == h * w\n",
    "        B, N, D = tokens.shape\n",
    "        feat = tokens.transpose(1, 2).reshape(B, D, h, w)  # (B, D, h, w)\n",
    "        kv = self.kv_conv(feat)  # (B, 2*D, h, w)\n",
    "        k_map, v_map = kv.chunk(2, dim=1)  # each (B, D, h, w)\n",
    "        return k_map, v_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.570288Z",
     "iopub.status.busy": "2025-09-10T13:40:09.569666Z",
     "iopub.status.idle": "2025-09-10T13:40:09.586213Z",
     "shell.execute_reply": "2025-09-10T13:40:09.585293Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.570262Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureSampler:\n",
    "    \"\"\"\n",
    "    Utility functions (not a nn.Module) to sample features at fractional positions using F.grid_sample.\n",
    "\n",
    "    - Sample per-head feature maps: we reshape feature maps into (B*H, C_head, h, w)\n",
    "    - Build grid and call F.grid_sample with align_corners=True to match normalization convention used.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def sample_maps_at_points(\n",
    "        feature_map: torch.Tensor,\n",
    "        sample_norm: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample feature_map at normalized points.\n",
    "\n",
    "        feature_map: (B * H, C_head, h, w)\n",
    "        sample_norm: (B * H, N * P, 2) in normalized coords [-1, 1], ordering (x, y) per point.\n",
    "        Returns:\n",
    "            sampled: (B, H, N, P, C_head)\n",
    "        \"\"\"\n",
    "        B_H, C_head, h, w = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        dtype = feature_map.dtype\n",
    "\n",
    "        # grid requires shape (N_batch, H_out, W_out, 2). We'll set H_out = N*P, W_out = 1\n",
    "        grid = sample_norm.view(B_H, -1, 1, 2)  # (B*H, N*P, 1, 2)\n",
    "\n",
    "        # grid_sample -> output (B*H, C_head, N*P, 1)\n",
    "        sampled = F.grid_sample(feature_map, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "        # reshape to (B, H, C_head, N, P, 1) -> drop last dim -> (B, H, C_head, N, P)\n",
    "        # we know B_H = B * H, so recover B and H later\n",
    "        # first reshape to (B, H, C_head, N, P)\n",
    "        # But we need N and P; they can be derived from sample_norm original leading dim: sample_norm.view(B_H, N*P, 2)\n",
    "        # So compute N*P:\n",
    "        _, NP, _ = sample_norm.shape  # NP = N * P\n",
    "        # We'll infer N by dividing with known P (passed via context in caller) — but to keep method generic,\n",
    "        # the caller should have arranged sample_norm with the correct ordering and will later reshape.\n",
    "        # Here we produce (B_H, C_head, NP, 1) -> reshape to (B, H, C_head, N, P) by caller's knowledge.\n",
    "        sampled = sampled.squeeze(-1)  # (B*H, C_head, NP)\n",
    "\n",
    "        return sampled  # (B*H, C_head, N*P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.588049Z",
     "iopub.status.busy": "2025-09-10T13:40:09.587293Z",
     "iopub.status.idle": "2025-09-10T13:40:09.615618Z",
     "shell.execute_reply": "2025-09-10T13:40:09.614729Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.588021Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeformableAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Readable, modular implementation of deformable attention.\n",
    "\n",
    "    Key features:\n",
    "    - Per-query, per-head learnable offsets (n_points per query per head)\n",
    "    - Bilinear sampling of K and V feature maps using F.grid_sample\n",
    "    - Dot-product attention between query and sampled keys (softmax over P)\n",
    "    - Output projected back to embedding dimension\n",
    "\n",
    "    Parameters:\n",
    "        dim: input & output embedding dimension\n",
    "        num_heads: number of attention heads\n",
    "        n_points: sampling points per query per head\n",
    "        offset_scale: scale multiplier for tanh-squashed offsets (in pixels)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, n_points: int = 9, offset_scale: float = 4.0, debug: bool = False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_points = n_points\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.offset_scale = float(offset_scale)\n",
    "        self.debug = debug\n",
    "\n",
    "        # modules\n",
    "        self.to_q = nn.Linear(dim, dim, bias=True)\n",
    "        self.offset_predictor = OffsetPredictor(dim, num_heads, n_points, offset_scale=offset_scale)\n",
    "        self.feature_producer = FeatureMapProducer(dim)\n",
    "        self.out_proj = nn.Linear(dim, dim, bias=True)\n",
    "\n",
    "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, N, D) -> (B, H, N, head_dim)\n",
    "        B, N, D = x.shape\n",
    "        return x.view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    def _merge_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, H, N, head_dim) -> (B, N, D)\n",
    "        B, H, N, hd = x.shape\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(B, N, H * hd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, N, D), where N == h * w\n",
    "        returns: (B, N, D)\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        assert N == h * w, \"N must equal h*w\"\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DeformableAttention] input: {x.shape}, h={h}, w={w}\")\n",
    "\n",
    "        # 1) Query projection -> (B, H, N, head_dim)\n",
    "        q = self.to_q(x)\n",
    "        q = self._split_heads(q)  # (B, H, N, hd)\n",
    "\n",
    "        # 2) Offsets -> (B, N, H, P, 2) in pixel units (dx, dy)\n",
    "        offsets = self.offset_predictor(x)  # (B, N, H, P, 2)\n",
    "\n",
    "        # 3) Feature maps (k_map, v_map) each (B, D, h, w)\n",
    "        k_map, v_map = self.feature_producer(x, h, w)  # (B, D, h, w)\n",
    "        if self.debug:\n",
    "            print(f\"[DeformableAttention] k_map shape: {k_map.shape}, v_map shape: {v_map.shape}\")\n",
    "\n",
    "        # 4) Split K and V per head: -> (B*H, head_dim, h, w)\n",
    "        B, D, _, _ = k_map.shape\n",
    "        k_map_heads = k_map.view(B, self.num_heads, self.head_dim, h, w).reshape(B * self.num_heads, self.head_dim, h, w)\n",
    "        v_map_heads = v_map.view(B, self.num_heads, self.head_dim, h, w).reshape(B * self.num_heads, self.head_dim, h, w)\n",
    "\n",
    "        # 5) Base grid (pixel coords) and absolute sample positions\n",
    "        base_xy = make_base_grid(h, w, device=device, dtype=dtype)  # (N, 2) with (x, y)\n",
    "        base_xy = base_xy.unsqueeze(0).expand(B, -1, -1)  # (B, N, 2)\n",
    "\n",
    "        # offsets currently (B, N, H, P, 2) in (dx, dy) ordering; grid_sample needs (x, y)\n",
    "        # swap to (dx, dy) -> (x,y) means ordering (dx, dy) corresponds to adding to base (x,y)\n",
    "        # we have offsets as (dx, dy) ordering already if OffsetPredictor produced (dx,dy)\n",
    "        sample_xy = base_xy.unsqueeze(2).unsqueeze(3) + offsets  # (B, N, H, P, 2)\n",
    "\n",
    "        # normalize sample coordinates to [-1, 1] for grid_sample (x normalized by w-1, y by h-1)\n",
    "        norm = torch.tensor([(w - 1), (h - 1)], device=device, dtype=dtype).view(1, 1, 1, 1, 2)\n",
    "        sample_norm = (sample_xy / norm) * 2.0 - 1.0  # (B, N, H, P, 2)\n",
    "\n",
    "        # 6) Rearrange sample_norm for grid_sample:\n",
    "        # desired ordering for sampling is (B, H, N*P, 2) -> then view to (B*H, N*P, 2)\n",
    "        sample_norm = sample_norm.permute(0, 2, 1, 3, 4).contiguous()  # (B, H, N, P, 2)\n",
    "        B_H = B * self.num_heads\n",
    "        sample_norm = sample_norm.view(B_H, N * self.n_points, 2)  # (B*H, N*P, 2)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DeformableAttention] sample_norm shape (for grid_sample): {sample_norm.shape}\")\n",
    "\n",
    "        # 7) Use grid_sample to fetch features at sampled points (bilinear interpolation)\n",
    "        # grid_sample expects (N_batch, C, h, w) and grid shaped (N_batch, H_out, W_out, 2),\n",
    "        # and returns (N_batch, C, H_out, W_out).\n",
    "        # We'll set H_out = N * P, W_out = 1\n",
    "        grid_for_gs = sample_norm.view(B_H, N * self.n_points, 1, 2)  # (B*H, N*P, 1, 2)\n",
    "        sampled_k = F.grid_sample(k_map_heads, grid_for_gs, mode='bilinear', padding_mode='zeros', align_corners=True)  # (B*H, hd, N*P, 1)\n",
    "        sampled_v = F.grid_sample(v_map_heads, grid_for_gs, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "        # remove last dim and reshape to (B, H, hd, N, P)\n",
    "        sampled_k = sampled_k.squeeze(-1).view(B, self.num_heads, self.head_dim, N, self.n_points)\n",
    "        sampled_v = sampled_v.squeeze(-1).view(B, self.num_heads, self.head_dim, N, self.n_points)\n",
    "\n",
    "        # permute to (B, H, N, P, hd)\n",
    "        sampled_k = sampled_k.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        sampled_v = sampled_v.permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DeformableAttention] sampled_k shape: {sampled_k.shape}, sampled_v shape: {sampled_v.shape}\")\n",
    "\n",
    "        # 8) Compute attention logits and weights\n",
    "        # q: (B, H, N, hd) ; sampled_k: (B, H, N, P, hd)\n",
    "        # compute dot product along hd -> (B, H, N, P)\n",
    "        # use einsum for clarity\n",
    "        attn_logits = torch.einsum('bhnd,bhnpd->bhnp', q, sampled_k) * (self.head_dim ** -0.5)\n",
    "        attn_weights = F.softmax(attn_logits, dim=-1)  # (B, H, N, P)\n",
    "\n",
    "        # 9) Weighted sum of sampled_v -> (B, H, N, hd)\n",
    "        out_heads = torch.einsum('bhnp,bhnpd->bhnd', attn_weights, sampled_v)  # (B, H, N, hd)\n",
    "\n",
    "        # 10) Merge heads and project\n",
    "        out = self._merge_heads(out_heads)  # (B, N, D)\n",
    "        out = self.out_proj(out)  # (B, N, D)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DeformableAttention] out shape: {out.shape}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.616933Z",
     "iopub.status.busy": "2025-09-10T13:40:09.616648Z",
     "iopub.status.idle": "2025-09-10T13:40:09.688374Z",
     "shell.execute_reply": "2025-09-10T13:40:09.687523Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.616915Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DLMWA(nn.Module):\n",
    "    \"\"\"\n",
    "    Local mixing block wrapping DeformableAttention with LayerNorm and MLP (residuals).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 2.0, n_points: int = 9, offset_scale: float = 4.0, debug: bool = False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = DeformableAttention(dim, num_heads, n_points=n_points, offset_scale=offset_scale, debug=debug)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, ratio=mlp_ratio)\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "        if self.debug:\n",
    "            print(f\"[DLMWA] input: {x.shape}\")\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, h, w)\n",
    "        x = res + x\n",
    "        res2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = res2 + x\n",
    "        if self.debug:\n",
    "            print(f\"[DLMWA] output: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "# example usage / test\n",
    "h, w = 32, 64\n",
    "B = 2\n",
    "dim = 64\n",
    "num_heads = 8\n",
    "n_points = 9\n",
    "\n",
    "dummy = torch.randn(B, h * w, dim)\n",
    "block = DLMWA(dim=dim, num_heads=num_heads, n_points=n_points, debug=False)\n",
    "out = block(dummy, h, w)\n",
    "print(\"Input shape:\", dummy.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "assert out.shape == dummy.shape, \"Output shape mismatch!\"\n",
    "print(\"Smoke test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_4h8--E8Az-"
   },
   "source": [
    "## Local mixing using a small depthwise conv (efficient alternative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.693217Z",
     "iopub.status.busy": "2025-09-10T13:40:09.692984Z",
     "iopub.status.idle": "2025-09-10T13:40:09.713890Z",
     "shell.execute_reply": "2025-09-10T13:40:09.713189Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.693197Z"
    },
    "id": "cRlnCe2d7y_l",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalMixingConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A fast local mixing implemented by depthwise convolution followed by pointwise projection.\n",
    "    Works as a local mixing alternative to window attention (useful for speed).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, kernel_size=(7,11), mlp_ratio=2.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        kh, kw = kernel_size\n",
    "        self.norm1 = nn.LayerNorm(dim) # Layer normalization before depthwise convolution\n",
    "        # Depthwise convolution: applies a separate convolution to each input channel\n",
    "        self.dw = nn.Conv2d(dim, dim, kernel_size=(kh, kw), padding=(kh//2, kw//2), groups=dim)\n",
    "        # Pointwise convolution: 1x1 convolution to mix information across channels\n",
    "        self.pw = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "        self.norm2 = nn.LayerNorm(dim) # Layer normalization before MLP\n",
    "        self.mlp = MLP(dim, ratio=mlp_ratio, drop=drop) # MLP block\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        # x: (B, N, D) where B is batch size, N is sequence length (h*w), D is dimension\n",
    "        B, N, D = x.shape\n",
    "        assert N == h * w, \"N must equal h*w\" # Assert sequence length matches spatial dimensions\n",
    "        res = x # Residual connection\n",
    "\n",
    "        # Pre-normalization\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Reshape to NCHW for convolution (B, D, h, w)\n",
    "        x2 = x.transpose(1, 2).reshape(B, D, h, w)\n",
    "\n",
    "        # Apply depthwise and pointwise convolutions\n",
    "        x2 = self.dw(x2)\n",
    "        x2 = self.pw(x2)\n",
    "\n",
    "        # Reshape back to (B, N, D)\n",
    "        x = x2.view(B, D, h * w).transpose(1, 2).contiguous()\n",
    "\n",
    "        # Add residual connection after convolution\n",
    "        x = res + x\n",
    "\n",
    "        # Second residual connection\n",
    "        res = x\n",
    "\n",
    "        # Pre-normalization before MLP\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and add residual connection\n",
    "        x = res + self.mlp(x)\n",
    "\n",
    "        return x # Output tensor with the same shape as input (B, N, D)\n",
    "\n",
    "# Test the LocalMixingConv module\n",
    "# The input to LocalMixingConv is (B, N, D), where N = h * w\n",
    "B, D = 2, 64 # Example Batch size and Dimension\n",
    "h, w = 32, 64 # Example spatial dimensions (after patch embedding)\n",
    "N = h * w # Sequence length\n",
    "dummy_input_conv = torch.randn(B, N, D)\n",
    "\n",
    "# Instantiate the LocalMixingConv module\n",
    "kernel_size = (7, 11) # Example kernel size\n",
    "local_mixing_conv = LocalMixingConv(dim=D, kernel_size=kernel_size)\n",
    "\n",
    "# Pass the dummy input through the module\n",
    "output_conv = local_mixing_conv(dummy_input_conv, h, w)\n",
    "\n",
    "print(f\"LocalMixingConv Input shape: {dummy_input_conv.shape}\")\n",
    "print(f\"LocalMixingConv Output shape: {output_conv.shape}\")\n",
    "\n",
    "# Verify that the output shape is the same as the input shape\n",
    "assert dummy_input_conv.shape == output_conv.shape, \"Output shape does not match input shape!\"\n",
    "print(\"Test passed: Output shape matches input shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8QMwgfT8uvn"
   },
   "source": [
    "## Merge / reduce resolution between stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.715109Z",
     "iopub.status.busy": "2025-09-10T13:40:09.714871Z",
     "iopub.status.idle": "2025-09-10T13:40:09.727698Z",
     "shell.execute_reply": "2025-09-10T13:40:09.726787Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.715085Z"
    },
    "id": "ct-e1g-L8rDh",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Merging(nn.Module):\n",
    "    \"\"\"\n",
    "    Merge reduces height by 2 (stride (2,1)) similar to SVTR merge layer.\n",
    "    Input x: (B, N, D) with h,w provided.\n",
    "    Returns x_new (B, N_new, D_out), and new (h, w).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=(2,1), padding=1)\n",
    "        self.norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        B, N, D = x.shape\n",
    "        x2 = x.transpose(1, 2).reshape(B, D, h, w)\n",
    "        x2 = self.conv(x2)  # (B, out_dim, h//2, w)\n",
    "        _, _, hp, wp = x2.shape\n",
    "        x = x2.flatten(2).transpose(1, 2).contiguous()  # (B, hp*wp, out_dim)\n",
    "        x = self.norm(x)\n",
    "        return x, hp, wp\n",
    "\n",
    "# Test the Merging module\n",
    "B, in_dim = 2, 12 # Example Batch size and Input Dimension\n",
    "out_dim = 123 # Example Output Dimension\n",
    "h, w = 32, 64 # Example spatial dimensions before merging\n",
    "N = h * w # Sequence length\n",
    "dummy_input_merge = torch.randn(B, N, in_dim)\n",
    "\n",
    "# Instantiate the Merging module\n",
    "merging_layer = Merging(in_dim=in_dim, out_dim=out_dim)\n",
    "\n",
    "# Pass the dummy input through the module\n",
    "output_merge, new_h, new_w = merging_layer(dummy_input_merge, h, w)\n",
    "\n",
    "print(f\"Merging Input shape: {dummy_input_merge.shape}\")\n",
    "print(f\"Merging Output shape: {output_merge.shape}\")\n",
    "print(f\"New spatial dimensions (h, w): ({new_h}, {new_w})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-NMywQZ9wYv"
   },
   "source": [
    "## Combine -> collapse height to 1 and produce sequence (B, W, D_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.728916Z",
     "iopub.status.busy": "2025-09-10T13:40:09.728656Z",
     "iopub.status.idle": "2025-09-10T13:40:09.749050Z",
     "shell.execute_reply": "2025-09-10T13:40:09.748147Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.728889Z"
    },
    "id": "oJCdao4W9Qrp",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Combining(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine across height: collapse height via mean and project channels to out_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        # x: (B, N, D) where N = h*w\n",
    "        B, N, D = x.shape\n",
    "        x2 = x.transpose(1, 2).reshape(B, D, h, w)  # (B, D, h, w)\n",
    "        # collapse height by mean -> (B, D, 1, w)\n",
    "        x2 = x2.mean(dim=2, keepdim=True)  # average over height\n",
    "        x2 = x2.squeeze(2)  # (B, D, w)\n",
    "        x2 = x2.transpose(1, 2).contiguous()  # (B, w, D)\n",
    "        x2 = self.fc(x2)\n",
    "        x2 = self.act(x2)\n",
    "        x2 = self.drop(x2)\n",
    "        return x2  # (B, w, out_dim)\n",
    "\n",
    "# Test the Combining module\n",
    "B, in_dim = 2, 12 # Example Batch size and Input Dimension (e.g., output of Merging)\n",
    "out_dim = 123 # Example Output Dimension\n",
    "# Assume input comes from a Merging layer that reduced height to 16 and kept width at 64\n",
    "h, w = 16, 64 # Example spatial dimensions before combining\n",
    "N = h * w # Sequence length\n",
    "dummy_input_combine = torch.randn(B, N, in_dim)\n",
    "\n",
    "# Instantiate the Combining module\n",
    "combining_layer = Combining(in_dim=in_dim, out_dim=out_dim)\n",
    "\n",
    "# Pass the dummy input through the module\n",
    "output_combine = combining_layer(dummy_input_combine, h, w)\n",
    "\n",
    "print(f\"Combining Input shape: {dummy_input_combine.shape}\")\n",
    "print(f\"Combining Output shape: {output_combine.shape}\")\n",
    "\n",
    "# Verify the output shape\n",
    "expected_output_shape = torch.Size([B, w, out_dim])\n",
    "assert output_combine.shape == expected_output_shape, \"Output shape does not match expected shape!\"\n",
    "print(\"Test passed: Output shape matches expected shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsRiDExd-ec3"
   },
   "source": [
    "## SVTR full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:09.750517Z",
     "iopub.status.busy": "2025-09-10T13:40:09.750167Z",
     "iopub.status.idle": "2025-09-10T13:40:10.517924Z",
     "shell.execute_reply": "2025-09-10T13:40:10.517282Z",
     "shell.execute_reply.started": "2025-09-10T13:40:09.750494Z"
    },
    "id": "X1G3Wmlt9z0l",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SVTR(nn.Module):\n",
    "    \"\"\"\n",
    "    SVTR-like model architecture (PyTorch implementation).\n",
    "\n",
    "    Args:\n",
    "        img_size (tuple): Input image size (height, width). Defaults to (64, 256).\n",
    "        in_chans (int): Number of input image channels. Defaults to 3.\n",
    "        vocab_size (int): Size of the output vocabulary (number of characters). Defaults to 100.\n",
    "        embed_dims (tuple): Embedding dimensions for each stage. Defaults to (64, 128, 256).\n",
    "        d3 (int): Output dimension of the combining layer before the head. Defaults to 192.\n",
    "        heads (tuple): Number of attention heads for each stage. Defaults to (2, 4, 8).\n",
    "        mlp_ratio (float): Ratio to determine hidden dimension in MLP. Defaults to 2.0.\n",
    "        window_sizes (list(tuple)): Window sizes for window-based local attention. length must match the number of 'L' blocks in the pattern.\n",
    "                             Also used as kernel size for LocalMixingConv.\n",
    "        num_blocks (tuple): Number of blocks in each stage. Defaults to (3, 6, 3).\n",
    "        pattern (list, optional): List of 'L' (local) or 'G' (global) specifying block types.\n",
    "                                  If None, a default pattern is generated.\n",
    "        local_type (list): List of local mixing types for each local block ('non_overlapping', 'swin', 'deformable', 'conv').\n",
    "                           Length must match the number of 'L' blocks in the pattern.\n",
    "        drop (float): Dropout rate. Defaults to 0.0.\n",
    "        n_points (int): Number of sampling points for Deformable Attention. Defaults to 9.\n",
    "        offset_scale (float): Scaling factor for Deformable Attention offsets. Defaults to 4.0.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size=(64, 256),\n",
    "                 in_chans=3,\n",
    "                 vocab_size=100,\n",
    "                 embed_dims=(64, 128, 256),\n",
    "                 d3=192,\n",
    "                 heads=(2, 4, 8),\n",
    "                 mlp_ratio=2.0,\n",
    "                 window_sizes=[(7, 11)] * 12 + [(3,3)] * 6, # Default window sizes if not provided\n",
    "                 num_blocks=(3, 6, 3),\n",
    "                 pattern=None,\n",
    "                 local_type=None,\n",
    "                 drop=0.0,\n",
    "                 n_points=9,\n",
    "                 offset_scale=4.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # pattern length must cover sum(num_blocks). L = local, G = global\n",
    "        total_blocks = sum(num_blocks)\n",
    "        assert pattern is not None and len(pattern) == total_blocks, f\"Pattern must be a list of length {total_blocks} specifying 'L' or 'G' for each block.\"\n",
    "\n",
    "        # Ensure local_type is a list and matches the number of local blocks in the pattern\n",
    "        assert isinstance(local_type, list), \"local_type must be a list specifying block types.\"\n",
    "        num_local_blocks = pattern.count('L')\n",
    "        assert len(local_type) >= num_local_blocks, f\"Length of local_type list ({len(local_type)}) must match the number of local blocks in pattern ({num_local_blocks}).\"\n",
    "        self.local_type_list = local_type\n",
    "\n",
    "        # Ensure window_sizes list length matches the number of local blocks\n",
    "        assert isinstance(window_sizes, list), \"window_sizes must be a list of tuples.\"\n",
    "        assert len(window_sizes) >= num_local_blocks, f\"Length of window_sizes list ({len(window_sizes)}) must match the number of local blocks in pattern ({num_local_blocks}).\"\n",
    "        self.window_sizes_list = window_sizes\n",
    "\n",
    "\n",
    "        self.patch_embed = PatchEmbedSVTR(img_size, in_chans, embed_dim=embed_dims[0])\n",
    "        self.patches_resolution = self.patch_embed.patches_resolution\n",
    "        dims = list(embed_dims) # Convert to list\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches, dims[0]))  # learnable positional embedding\n",
    "        self.n_points = n_points # Store for Deformable Attention\n",
    "        self.offset_scale = offset_scale # Store for Deformable Attention\n",
    "        self.drop_rate = drop # Store dropout rate\n",
    "\n",
    "        # Build the model stages based on the pattern and local_type\n",
    "        cur_pattern_idx = 0\n",
    "        cur_local_type_idx = 0\n",
    "        cur_window_size_idx = 0\n",
    "\n",
    "        # Stage 1\n",
    "        self.stage1 = nn.ModuleList()\n",
    "        for i in range(num_blocks[0]):\n",
    "            tp = pattern[cur_pattern_idx]\n",
    "            if tp == 'L':\n",
    "                current_local_type = self.local_type_list[cur_local_type_idx]\n",
    "                current_window_size = self.window_sizes_list[cur_window_size_idx]\n",
    "                if current_local_type == 'non_overlapping':\n",
    "                    blk = NOLMWA(dims[0], heads[0], mlp_ratio, current_window_size, self.drop_rate)\n",
    "                elif current_local_type == 'swin':\n",
    "                    # Alternate shift for SWIN\n",
    "                    do_shift = (i % 2 == 1)\n",
    "                    shift_size = (current_window_size[0] // 2, current_window_size[1] // 2) if do_shift else (0, 0)\n",
    "                    blk = SWLMWA(dims[0], heads[0], mlp_ratio, current_window_size, self.drop_rate, shift_size)\n",
    "                elif current_local_type == 'deformable':\n",
    "                    blk = DLMWA(dims[0], heads[0], mlp_ratio, self.n_points, self.offset_scale, debug=False) # debug=False by default\n",
    "                elif current_local_type == 'conv':\n",
    "                    blk = LocalMixingConv(dims[0], kernel_size=current_window_size, mlp_ratio=mlp_ratio, drop=self.drop_rate)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown local_type '{current_local_type}' for block {cur_pattern_idx}\")\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else: # Global mixing\n",
    "                blk = GlobalMixing(dims[0], heads[0], mlp_ratio, self.drop_rate)\n",
    "            self.stage1.append(blk)\n",
    "            cur_pattern_idx += 1\n",
    "\n",
    "        self.merge1 = Merging(dims[0], dims[1])\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = nn.ModuleList()\n",
    "        for i in range(num_blocks[1]):\n",
    "            tp = pattern[cur_pattern_idx]\n",
    "            if tp == 'L':\n",
    "                current_local_type = self.local_type_list[cur_local_type_idx]\n",
    "                current_window_size = self.window_sizes_list[cur_window_size_idx]\n",
    "                if current_local_type == 'non_overlapping':\n",
    "                    blk = NOLMWA(dims[1], heads[1], mlp_ratio, current_window_size, self.drop_rate)\n",
    "                elif current_local_type == 'swin':\n",
    "                    do_shift = (i % 2 == 1)\n",
    "                    shift_size = (current_window_size[0] // 2, current_window_size[1] // 2) if do_shift else (0, 0)\n",
    "                    blk = SWLMWA(dims[1], heads[1], mlp_ratio, current_window_size, self.drop_rate, shift_size)\n",
    "                elif current_local_type == 'deformable':\n",
    "                    blk = DLMWA(dims[1], heads[1], mlp_ratio, self.n_points, self.offset_scale, debug=False)\n",
    "                elif current_local_type == 'conv':\n",
    "                    blk = LocalMixingConv(dims[1], kernel_size=current_window_size, mlp_ratio=mlp_ratio, drop=self.drop_rate)\n",
    "                else:\n",
    "                     raise ValueError(f\"Unknown local_type '{current_local_type}' for block {cur_pattern_idx}\")\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else: # Global mixing\n",
    "                blk = GlobalMixing(dims[1], heads[1], mlp_ratio, self.drop_rate)\n",
    "            self.stage2.append(blk)\n",
    "            cur_pattern_idx += 1\n",
    "\n",
    "        self.merge2 = Merging(dims[1], dims[2])\n",
    "\n",
    "        # Stage 3\n",
    "        self.stage3 = nn.ModuleList()\n",
    "        for i in range(num_blocks[2]):\n",
    "            tp = pattern[cur_pattern_idx]\n",
    "            if tp == 'L':\n",
    "                current_local_type = self.local_type_list[cur_local_type_idx]\n",
    "                current_window_size = self.window_sizes_list[cur_window_size_idx]\n",
    "                if current_local_type == 'non_overlapping':\n",
    "                    blk = NOLMWA(dims[2], heads[2], mlp_ratio, current_window_size, self.drop_rate)\n",
    "                elif current_local_type == 'swin':\n",
    "                    do_shift = (i % 2 == 1)\n",
    "                    shift_size = (current_window_size[0] // 2, current_window_size[1] // 2) if do_shift else (0, 0)\n",
    "                    blk = SWLMWA(dims[2], heads[2], mlp_ratio, current_window_size, self.drop_rate, shift_size)\n",
    "                elif current_local_type == 'deformable':\n",
    "                    blk = DLMWA(dims[2], heads[2], mlp_ratio, self.n_points, self.offset_scale, debug=False)\n",
    "                elif current_local_type == 'conv':\n",
    "                    blk = LocalMixingConv(dims[2], kernel_size=current_window_size, mlp_ratio=mlp_ratio, drop=self.drop_rate)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown local_type '{current_local_type}' for block {cur_pattern_idx}\")\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else: # Global mixing\n",
    "                blk = GlobalMixing(dims[2], heads[2], mlp_ratio, self.drop_rate)\n",
    "            self.stage3.append(blk)\n",
    "            cur_pattern_idx += 1\n",
    "\n",
    "        self.combine = Combining(dims[2], d3, drop=self.drop_rate)\n",
    "        self.head = nn.Linear(d3, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model.\n",
    "        Uses Kaiming uniform initialization for convolutional and linear layers,\n",
    "        constant initialization for batch norm and layer norm,\n",
    "        and truncated normal for positional embeddings.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, PatchEmbedSVTR):\n",
    "                        # Initialize convolution layers within PatchEmbedSVTR\n",
    "                        for conv in [m.conv1, m.conv2]:  # Adjust based on actual structure\n",
    "                            if isinstance(conv, nn.Conv2d):\n",
    "                                nn.init.kaiming_uniform_(conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "                                if conv.bias is not None:\n",
    "                                    nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "        # Initialize positional embedding with truncated normal distribution\n",
    "        if self.pos_embed is not None:\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the SVTR model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits for each character at each timestep (B, W, vocab_size).\n",
    "        \"\"\"\n",
    "        # x: Input image tensor (B, C, H, W)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch Embedding\n",
    "        # Input shape: (B, C, H, W) e.g., (B, 3, 64, 512)\n",
    "        x = self.patch_embed(x)\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        # Output shape: (B, N, D) where N is number of patches, D is embed_dim[0]\n",
    "        # N = (H/4) * (W/4)\n",
    "        h, w = self.patch_embed.patches_resolution # Spatial dimensions after patch embedding (H/4, W/4)\n",
    "\n",
    "        # Build the model stages based on the pattern and local_type\n",
    "        cur_pattern_idx = 0\n",
    "        cur_local_type_idx = 0\n",
    "        cur_window_size_idx = 0\n",
    "\n",
    "        # Stage 1\n",
    "        # Input shape: (B, N, embed_dims[0]) e.g., (B, 2048, 64) from patches_resolution (16, 128)\n",
    "        for i in range(len(self.stage1)):\n",
    "            blk = self.stage1[i]\n",
    "            tp = config['pattern'][cur_pattern_idx] # Get pattern from config\n",
    "            if tp == 'L':\n",
    "                # Local mixing requires spatial dimensions h, w\n",
    "                x = blk(x, h, w)\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else:\n",
    "                # Global mixing operates on the sequence (B, N, D)\n",
    "                x = blk(x)\n",
    "            cur_pattern_idx += 1\n",
    "            # Output shape after each block: (B, N, embed_dims[0])\n",
    "\n",
    "        # Merge 1\n",
    "        # Input shape: (B, N, embed_dims[0])\n",
    "        x, h, w = self.merge1(x, h, w)\n",
    "        # Output shape: (B, N_new, embed_dims[1]) and new spatial dimensions (h, w)\n",
    "        # Merging reduces height by 2, keeps width (approximately due to convolution stride)\n",
    "        # e.g., (B, (h//2)*w, embed_dims[1])\n",
    "\n",
    "        # Stage 2\n",
    "        # Input shape: (B, N_new, embed_dims[1])\n",
    "        for i in range(len(self.stage2)):\n",
    "            blk = self.stage2[i]\n",
    "            tp = config['pattern'][cur_pattern_idx] # Get pattern from config\n",
    "            if tp == 'L':\n",
    "                x = blk(x, h, w)\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else:\n",
    "                x = blk(x)\n",
    "            cur_pattern_idx += 1\n",
    "            # Output shape after each block: (B, N_new, embed_dims[1])\n",
    "\n",
    "        # Merge 2\n",
    "        # Input shape: (B, N_new, embed_dims[1])\n",
    "        x, h, w = self.merge2(x, h, w)\n",
    "        # Output shape: (B, N_new_new, embed_dims[2]) and new spatial dimensions (h, w)\n",
    "        # Merging reduces height by 2, keeps width\n",
    "\n",
    "        # Stage 3\n",
    "        # Input shape: (B, N_new_new, embed_dims[2])\n",
    "        for i in range(len(self.stage3)):\n",
    "            blk = self.stage3[i]\n",
    "            tp = config['pattern'][cur_pattern_idx] # Get pattern from config\n",
    "            if tp == 'L':\n",
    "                x = blk(x, h, w)\n",
    "                cur_local_type_idx += 1\n",
    "                cur_window_size_idx += 1\n",
    "            else:\n",
    "                x = blk(x)\n",
    "            cur_pattern_idx += 1\n",
    "            # Output shape after each block: (B, N_new_new, embed_dims[2])\n",
    "\n",
    "        # Combine -> collapse height to sequence of width length\n",
    "        # Input shape: (B, N_final, embed_dims[2]) where N_final is h*w after last merge\n",
    "        x = self.combine(x, h, w)\n",
    "        # Output shape: (B, w, d3)\n",
    "        # After combining, sequence length is the final width (w), dimension is d3.\n",
    "\n",
    "        # Final Linear Head (Classifier/Decoder)\n",
    "        # Input shape: (B, w, d3)\n",
    "        x = self.head(x)\n",
    "        # Output shape: (B, w, vocab_size)\n",
    "        # This is the output sequence of logits for each character at each timestep (width position).\n",
    "\n",
    "        return x\n",
    "\n",
    "model = SVTR(\n",
    "    img_size=(config['img_height'], config['img_width']),\n",
    "    in_chans=config['channels'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    local_type=config['local_type'], # Use the list from config\n",
    "    embed_dims=config['embed_dims'], # Use embed_dims from config\n",
    "    heads=config['heads'], # Use heads from config\n",
    "    mlp_ratio=config['mlp_ratio'], # Use mlp_ratio from config\n",
    "    window_sizes=config['window_sizes'], # Use window_sizes from config\n",
    "    num_blocks=config['num_blocks'], # Use num_blocks from config\n",
    "    pattern=config['pattern'], # Use pattern from config\n",
    "    drop=config['dropout_rate'], # Use dropout_rate from config\n",
    "    n_points=config['n_points'], # Use n_points from config\n",
    "    offset_scale=config['offset_scale'], # Use offset_scale from config\n",
    ").to(config['device']).eval()\n",
    "\n",
    "dummy = torch.randn(2, config['channels'], config['img_height'], config['img_width'], device=config['device'])\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "\n",
    "print(f\"Input shape: {dummy.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "# The expected output shape is (batch_size, final_width, vocab_size)\n",
    "# From the summary, the final width is 128.\n",
    "assert out.shape == torch.zeros((2, config['img_width'] // 4, config['vocab_size'])).shape, \"Output shape does not match expected shape!\"\n",
    "print(\"Test passed: Output shape matches expected shape.\")\n",
    "\n",
    "summary(model, input_size=dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFedscA1gbHj"
   },
   "source": [
    "# SECTION 8: COMPLETE TRAINING & VALIDATION & TESTING WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKE2j2Dbhn7R"
   },
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3iWvDEhqhe"
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.519607Z",
     "iopub.status.busy": "2025-09-10T13:40:10.518838Z",
     "iopub.status.idle": "2025-09-10T13:40:10.530285Z",
     "shell.execute_reply": "2025-09-10T13:40:10.529368Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.519585Z"
    },
    "id": "33gNUw5WdZAd",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class OCRMetrics:\n",
    "    \"\"\"Comprehensive evaluation metrics for OCR\"\"\"\n",
    "\n",
    "    def __init__(self, text_processor):\n",
    "        self.text_processor = text_processor\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_samples = 0\n",
    "        self.correct_samples = 0\n",
    "        self.total_chars = 0\n",
    "        self.correct_chars = 0\n",
    "        self.total_edit_distance = 0\n",
    "        self.total_word_distance = 0\n",
    "        self.total_words = 0\n",
    "        self.correct_words = 0\n",
    "        self.char_error_rates = []\n",
    "        self.word_error_rates = []\n",
    "        self.sequence_accuracies = []\n",
    "\n",
    "    def update(self, predictions, targets):\n",
    "        \"\"\"Update metrics with batch predictions and targets\"\"\"\n",
    "        batch_size = len(predictions)\n",
    "        self.total_samples += batch_size\n",
    "\n",
    "        for pred_text, target_text in zip(predictions, targets):\n",
    "            # Character-level metrics\n",
    "            pred_chars = list(pred_text)\n",
    "            target_chars = list(target_text)\n",
    "\n",
    "            self.total_chars += len(target_chars)\n",
    "\n",
    "            # Character accuracy\n",
    "            correct_chars = sum(1 for p, t in zip(pred_chars, target_chars) if p == t)\n",
    "            self.correct_chars += correct_chars\n",
    "\n",
    "            # Character Error Rate (CER)\n",
    "            edit_dist = editdistance.eval(pred_text, target_text)\n",
    "            self.total_edit_distance += edit_dist\n",
    "            cer = edit_dist / max(len(target_text), 1)\n",
    "            self.char_error_rates.append(cer)\n",
    "\n",
    "            # Word-level metrics\n",
    "            pred_words = pred_text.split()\n",
    "            target_words = target_text.split()\n",
    "\n",
    "            self.total_words += len(target_words)\n",
    "\n",
    "            # Word accuracy\n",
    "            correct_words = sum(1 for p, t in zip(pred_words, target_words) if p == t)\n",
    "            self.correct_words += correct_words\n",
    "\n",
    "            # Word Error Rate (WER)\n",
    "            word_edit_dist = editdistance.eval(pred_words, target_words)\n",
    "            self.total_word_distance += word_edit_dist\n",
    "            wer = word_edit_dist / max(len(target_words), 1)\n",
    "            self.word_error_rates.append(wer)\n",
    "\n",
    "            # Sequence accuracy\n",
    "            if pred_text == target_text:\n",
    "                self.correct_samples += 1\n",
    "                self.sequence_accuracies.append(1.0)\n",
    "            else:\n",
    "                self.sequence_accuracies.append(0.0)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"Calculate and return all metrics\"\"\"\n",
    "        if self.total_samples == 0:\n",
    "            return {}\n",
    "\n",
    "        metrics = {\n",
    "            'sequence_accuracy': self.correct_samples / self.total_samples,\n",
    "            'character_accuracy': self.correct_chars / self.total_chars,\n",
    "            'word_accuracy': self.correct_words / self.total_words,\n",
    "            'character_error_rate': self.total_edit_distance / self.total_chars,\n",
    "            'word_error_rate': self.total_word_distance / self.total_words,\n",
    "            'avg_cer': np.mean(self.char_error_rates),\n",
    "            'avg_wer': np.mean(self.word_error_rates),\n",
    "            'avg_sequence_accuracy': np.mean(self.sequence_accuracies),\n",
    "            'total_samples': self.total_samples\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.531749Z",
     "iopub.status.busy": "2025-09-10T13:40:10.531192Z",
     "iopub.status.idle": "2025-09-10T13:40:10.556171Z",
     "shell.execute_reply": "2025-09-10T13:40:10.555509Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.531722Z"
    },
    "id": "k_nmeAkHhd_o",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ctc_decode(logits, text_processor, beam_size=4):\n",
    "    \"\"\"Decode CTC output to text with beam search\"\"\"\n",
    "    # Get the most likely character at each time step\n",
    "    if beam_size == 1:  # Greedy decoding\n",
    "        pred_indices = torch.argmax(logits, dim=-1)  # [T, B]\n",
    "        pred_indices = pred_indices.transpose(0, 1)  # [B, T]\n",
    "\n",
    "        decoded_texts = []\n",
    "        for seq in pred_indices:\n",
    "            # Remove duplicates and blank tokens\n",
    "            decoded = []\n",
    "            prev_token = None\n",
    "            for token in seq:\n",
    "                token = token.item()\n",
    "                if token != text_processor.char2idx[text_processor.BLANK_TOKEN] and token != prev_token:\n",
    "                    decoded.append(token)\n",
    "                prev_token = token\n",
    "\n",
    "            # Convert to text\n",
    "            text = text_processor.decode_sequence(decoded)\n",
    "            decoded_texts.append(text)\n",
    "\n",
    "        return decoded_texts\n",
    "    else:\n",
    "        # Beam search decoding (simplified version)\n",
    "        # For full beam search, you'd need a more complex implementation\n",
    "        return ctc_decode(logits, text_processor, beam_size=1)  # Fallback to greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.557257Z",
     "iopub.status.busy": "2025-09-10T13:40:10.557017Z",
     "iopub.status.idle": "2025-09-10T13:40:10.577943Z",
     "shell.execute_reply": "2025-09-10T13:40:10.577120Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.557239Z"
    },
    "id": "iHpEKP8KhgfT",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024**3)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.579615Z",
     "iopub.status.busy": "2025-09-10T13:40:10.579046Z",
     "iopub.status.idle": "2025-09-10T13:40:10.599636Z",
     "shell.execute_reply": "2025-09-10T13:40:10.598840Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.579586Z"
    },
    "id": "ZujVyYixi0uj",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ArabicOCRInference:\n",
    "    \"\"\"Robust inference pipeline for Arabic OCR\"\"\"\n",
    "\n",
    "    def __init__(self, model, text_processor, device, transform=None):\n",
    "        self.model = model\n",
    "        self.text_processor = text_processor\n",
    "        self.device = device\n",
    "        self.transform = transform or get_val_transforms(config['dataset_mean'], config['dataset_std'])\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess single image for inference\"\"\"\n",
    "        # Handle different input types: file path (str), PIL Image, numpy array, or torch.Tensor\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image)\n",
    "\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image) # Convert PIL Image to numpy array\n",
    "\n",
    "        # At this point, image should be either a numpy array or a torch.Tensor\n",
    "        if isinstance(image, np.ndarray):\n",
    "             # Apply transforms if it's a numpy array (e.g., from file or PIL)\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image'] # Output of ToTensorV2 is a torch.Tensor\n",
    "\n",
    "            # Ensure it's a PyTorch tensor and add batch dimension\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                 image = torch.from_numpy(image)\n",
    "            image = image.unsqueeze(0) # Add batch dimension\n",
    "\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            # If the input was already a tensor, just ensure correct dimensions and dtype\n",
    "            # Assuming the input tensor is already in (C, H, W) format and correct dtype (float32)\n",
    "            # If not, additional checks/conversions might be needed\n",
    "            if len(image.shape) == 3: # C, H, W\n",
    "                image = image.unsqueeze(0) # Add batch dimension B, C, H, W\n",
    "            elif len(image.shape) == 4: # B, C, H, W\n",
    "                 pass # Already has batch dimension\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected tensor shape: {image.shape}. Expected (C, H, W) or (B, C, H, W).\")\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Supported types are str (file path), PIL.Image.Image, np.ndarray, or torch.Tensor.\")\n",
    "\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def predict(self, image, beam_size=4):\n",
    "        \"\"\"Predict text from single image\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Preprocess\n",
    "            input_tensor = self.preprocess_image(image).to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(input_tensor)\n",
    "\n",
    "            # Decode\n",
    "            # logits shape is (B, W, vocab_size), ctc_decode expects (T, B, C)\n",
    "            # T is sequence length (width W), B is batch size, C is vocab_size\n",
    "            decoded_texts = ctc_decode(logits.transpose(0, 1), self.text_processor, beam_size)\n",
    "\n",
    "            return decoded_texts[0] if decoded_texts else \"\" # Return the first (and only) prediction\n",
    "\n",
    "    def predict_batch(self, images, beam_size=4):\n",
    "        \"\"\"Predict text from batch of images\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Preprocess batch\n",
    "            batch_tensors = []\n",
    "            for image in images:\n",
    "                tensor = self.preprocess_image(image)\n",
    "                batch_tensors.append(tensor)\n",
    "\n",
    "            # Ensure all tensors have the same shape before stacking (except batch dimension)\n",
    "            # This might require padding or resizing images to a consistent size\n",
    "            # For now, assuming images are already of compatible size after individual preprocessing.\n",
    "            # A more robust implementation would handle variable sizes.\n",
    "            if not batch_tensors:\n",
    "                 return [] # Return empty list if no images were successfully preprocessed\n",
    "\n",
    "            # Check if all tensors have the same shape (excluding batch dim)\n",
    "            first_shape = batch_tensors[0].shape[1:]\n",
    "            if not all(t.shape[1:] == first_shape for t in batch_tensors):\n",
    "                # If shapes are inconsistent, pad or resize them\n",
    "                # For simplicity here, let's assume they are consistent or raise an error\n",
    "                # A better approach would involve padding the batch\n",
    "                 raise ValueError(\"Images in batch have inconsistent shapes after preprocessing.\")\n",
    "\n",
    "\n",
    "            batch_tensor = torch.cat(batch_tensors, dim=0).to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(batch_tensor) # Shape: (B, W, vocab_size)\n",
    "\n",
    "            # Decode\n",
    "            # logits shape is (B, W, vocab_size), ctc_decode expects (T, B, C)\n",
    "            decoded_texts = ctc_decode(logits.transpose(0, 1), self.text_processor, beam_size)\n",
    "\n",
    "            return decoded_texts\n",
    "\n",
    "    def predict_with_confidence(self, image, beam_size=4):\n",
    "        \"\"\"Predict text with confidence score\"\"\"\n",
    "        with torch.no_grad():\n",
    "            input_tensor = self.preprocess_image(image).to(self.device)\n",
    "            logits = self.model(input_tensor)\n",
    "\n",
    "            # Get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # Shape: (B, W, vocab_size)\n",
    "            # Get max probability for each time step\n",
    "            max_probs, _ = torch.max(probs, dim=-1) # Shape: (B, W)\n",
    "\n",
    "            # Average confidence across sequence for the first sample in the batch\n",
    "            # Assuming batch size is 1 for single image inference\n",
    "            confidence = torch.mean(max_probs[0]).item() if max_probs.size(0) > 0 else 0.0\n",
    "\n",
    "            # Decode\n",
    "            decoded_text = ctc_decode(logits.transpose(0, 1), self.text_processor, beam_size)\n",
    "\n",
    "            return decoded_text[0] if decoded_text else \"\", confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.601053Z",
     "iopub.status.busy": "2025-09-10T13:40:10.600731Z",
     "iopub.status.idle": "2025-09-10T13:40:10.621644Z",
     "shell.execute_reply": "2025-09-10T13:40:10.620787Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.601023Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plots training and validation loss and metrics over epochs.\"\"\"\n",
    "    print(\"\\nVisualizing training history...\")\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Get list of all metric names from history (assuming consistent metrics)\n",
    "    if history['train_metrics']:\n",
    "        metric_names = history['train_metrics'][0].keys()\n",
    "        # Exclude metrics that don't make sense to plot over time (like total_samples)\n",
    "        metrics_to_plot = [name for name in metric_names if name not in ['total_samples']]\n",
    "\n",
    "        for metric_name in metrics_to_plot:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            train_values = [m[metric_name] for m in history['train_metrics']]\n",
    "            val_values = [m[metric_name] for m in history['val_metrics']]\n",
    "\n",
    "            plt.plot(train_values, label=f'Train {metric_name}')\n",
    "            plt.plot(val_values, label=f'Validation {metric_name}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(metric_name)\n",
    "            plt.title(f'Training and Validation {metric_name} over Epochs')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.623205Z",
     "iopub.status.busy": "2025-09-10T13:40:10.622676Z",
     "iopub.status.idle": "2025-09-10T13:40:10.641545Z",
     "shell.execute_reply": "2025-09-10T13:40:10.640834Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.623176Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_error_analysis(error_analysis, dataset_name=\"Test Set\", num_examples=10):\n",
    "    \"\"\"Displays summary and sample error predictions.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{dataset_name} Detailed Analysis:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total incorrect samples in {dataset_name}: {len(error_analysis['error_samples'])}\")\n",
    "\n",
    "    num_error_examples_to_show = min(num_examples, len(error_analysis['error_samples']))\n",
    "    print(f\"First {num_error_examples_to_show} incorrect predictions:\")\n",
    "    for i, (pred, target) in enumerate(error_analysis['error_samples'][:num_error_examples_to_show]):\n",
    "        # Prepare Arabic text for RTL display in print output\n",
    "        # display_pred_text_print = get_display(arabic_reshaper.reshape(pred))\n",
    "        # display_target_text_print = get_display(arabic_reshaper.reshape(target))\n",
    "        display_pred_text_print = pred\n",
    "        display_target_text_print = target\n",
    "        print(f\"  Sample {i+1}: Predicted='{display_pred_text_print}', Actual='{display_target_text_print}'\")\n",
    "\n",
    "def plot_error_distributions(error_analysis, dataset_name=\"Test Set\"):\n",
    "    \"\"\"Plots distribution of CER and WER.\"\"\"\n",
    "    print(f\"\\nVisualizing {dataset_name} Error Distributions:\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(error_analysis['cer_distribution'], bins=20, alpha=0.7, label='CER Distribution')\n",
    "    plt.hist(error_analysis['wer_distribution'], bins=20, alpha=0.7, label='WER Distribution')\n",
    "    plt.xlabel('Error Rate')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of CER and WER on {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.642783Z",
     "iopub.status.busy": "2025-09-10T13:40:10.642531Z",
     "iopub.status.idle": "2025-09-10T13:40:10.665231Z",
     "shell.execute_reply": "2025-09-10T13:40:10.664494Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.642764Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(dataloader, text_processor, inference_pipeline, num_samples=12, dataset_mean=None, dataset_std=None):\n",
    "    \"\"\"Visualizes samples with actual and predicted text.\"\"\"\n",
    "    print(\"\\nVisualizing samples with predictions...\")\n",
    "    batch = next(iter(dataloader))\n",
    "    images = batch['images'][:num_samples]\n",
    "    texts = batch['texts'][:num_samples]\n",
    "\n",
    "    # Prepare images for the inference pipeline\n",
    "    inference_images = []\n",
    "    for img_tensor in images:\n",
    "        # Denormalize, permute, convert to numpy, convert to uint8, then to PIL Image\n",
    "        img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Use provided mean and std for denormalization, fallback to config\n",
    "        mean_val = np.array(dataset_mean if dataset_mean is not None else config['dataset_mean'])\n",
    "        std_val = np.array(dataset_std if dataset_std is not None else config['dataset_std'])\n",
    "        img_np = img_np * std_val + mean_val\n",
    "\n",
    "        img_np = np.clip(img_np, 0, 1) * 255 # Scale to 0-255\n",
    "        img_np = img_np.astype(np.uint8) # Convert to uint8\n",
    "        inference_images.append(Image.fromarray(img_np))\n",
    "\n",
    "    # Predict text for the batch using the inference pipeline\n",
    "    predicted_texts = inference_pipeline.predict_batch(inference_images)\n",
    "\n",
    "    # Ensure num_samples does not exceed the number of actual predictions\n",
    "    num_samples_to_show = min(num_samples, len(predicted_texts))\n",
    "\n",
    "\n",
    "    # Determine grid size (e.g., 4 columns, rows based on num_samples)\n",
    "    cols = 4\n",
    "    rows = (num_samples_to_show + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, rows * 5))\n",
    "    axes = axes.ravel() if isinstance(axes, np.ndarray) else [axes] # Handle single subplot case\n",
    "\n",
    "    for i in range(num_samples_to_show):\n",
    "        # Use the PIL Image for imshow\n",
    "        image = inference_images[i]\n",
    "        actual_text = texts[i]\n",
    "        predicted_text = predicted_texts[i]\n",
    "\n",
    "        # Prepare Arabic text for RTL display in plot titles\n",
    "        display_actual_text_plot = get_display(arabic_reshaper.reshape(actual_text))\n",
    "        display_predicted_text_plot = get_display(arabic_reshaper.reshape(predicted_text))\n",
    "\n",
    "        axes[i].imshow(image)\n",
    "        # Ensure text is displayed RTL using the prepared strings\n",
    "        axes[i].set_title(f\"Actual: {display_actual_text_plot}\\nPredicted: {display_predicted_text_plot}\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_samples_to_show, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqYwIAr1huef"
   },
   "source": [
    "### Epoch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.666832Z",
     "iopub.status.busy": "2025-09-10T13:40:10.666256Z",
     "iopub.status.idle": "2025-09-10T13:40:10.688326Z",
     "shell.execute_reply": "2025-09-10T13:40:10.687414Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.666809Z"
    },
    "id": "uqTXR6SuhisD",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, text_processor,\n",
    "                epoch, total_epochs, use_amp=True, grad_clip=1.0):\n",
    "    \"\"\"Complete training epoch with progress tracking and AMP support\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Clear CUDA cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize GradScaler for mixed precision training, and use mixed precision for memory efficiency\n",
    "    use_amp = torch.cuda.is_available() # Only enable AMP if CUDA is available\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "    # Progress bar with leave=True for better visualization\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch}/{total_epochs}\",\n",
    "                       leave=True, dynamic_ncols=True)\n",
    "\n",
    "    batch_times = []\n",
    "    metrics = OCRMetrics(text_processor)\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get batch data\n",
    "            images = batch['images'].to(device)\n",
    "            encoded_texts = batch['encoded_texts'].to(device)\n",
    "            text_lengths = batch['text_lengths'].to(device)\n",
    "            original_texts = batch['texts']\n",
    "\n",
    "            # Explicitly delete the batch from CPU memory after moving to GPU\n",
    "            del batch\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialize loss for the current batch\n",
    "            loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                logits = model(images)  # Shape: (B, W, vocab_size)\n",
    "\n",
    "                # Prepare CTC targets\n",
    "                batch_size = images.size(0)\n",
    "                # seq_length = logits.size(0) # Incorrect for CTC loss\n",
    "                seq_length = logits.size(1) # Correct sequence length (width W)\n",
    "\n",
    "                # Create mask for non-padding tokens\n",
    "                max_target_length = encoded_texts.size(1)\n",
    "                mask = torch.arange(max_target_length).expand(batch_size, max_target_length).to(device) < text_lengths.unsqueeze(1)\n",
    "                targets = encoded_texts[mask]\n",
    "                target_lengths = text_lengths\n",
    "                input_lengths = torch.full(size=(batch_size,), fill_value=seq_length, dtype=torch.long).to(device)\n",
    "\n",
    "                # Calculate CTC loss\n",
    "                # log_probs = F.log_softmax(logits, dim=2) # Shape: (B, W, vocab_size)\n",
    "                # Transpose log_probs to (W, B, vocab_size) for CTCLoss\n",
    "                log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
    "                # Ensure target_lengths match the effective batch size after masking\n",
    "                # and input_lengths matches the batch size of images.\n",
    "                # Also, ensure targets are long and on the correct device.\n",
    "                loss = criterion(log_probs, targets.long(), input_lengths, target_lengths.long())\n",
    "\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            batch_time = time.time() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "            # Decode predictions for metrics - logits shape expected by ctc_decode is (T, B, C)\n",
    "            # Need to transpose logits back to (W, B, vocab_size) before decoding\n",
    "            decoded_texts = ctc_decode(logits.transpose(0, 1), text_processor)\n",
    "            metrics.update(decoded_texts, original_texts)\n",
    "\n",
    "            # Explicitly delete intermediate tensors\n",
    "            del logits, log_probs, targets, mask, input_lengths\n",
    "            # Also delete images and encoded_texts as they are not needed anymore\n",
    "            del images, encoded_texts, text_lengths, original_texts\n",
    "\n",
    "\n",
    "            # Update progress bar\n",
    "            current_metrics = metrics.get_metrics()\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}',\n",
    "                'CER': f'{current_metrics.get(\"avg_cer\", 0):.4f}',\n",
    "                'Acc': f'{current_metrics.get(\"sequence_accuracy\", 0):.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}',\n",
    "                'Time': f'{batch_time:.3f}s',\n",
    "                'Mem': f'{get_gpu_memory_usage():.1f}GB' if torch.cuda.is_available() else 'N/A'\n",
    "            })\n",
    "\n",
    "            # Clear some memory periodically\n",
    "            if batch_idx % 50 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print(f\"OOM error at batch {batch_idx}. Skipping batch...\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                # Continue to the next batch\n",
    "                continue\n",
    "            else:\n",
    "                # Re-raise other RuntimeErrors\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "             # Catch any other unexpected errors during batch processing\n",
    "             print(f\"Error processing batch {batch_idx}: {e}. Skipping batch...\")\n",
    "             if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "             continue\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_batch_time = np.mean(batch_times) if batch_times else 0.0\n",
    "    final_metrics = metrics.get_metrics()\n",
    "\n",
    "    return avg_loss, final_metrics, avg_batch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.689610Z",
     "iopub.status.busy": "2025-09-10T13:40:10.689313Z",
     "iopub.status.idle": "2025-09-10T13:40:10.711258Z",
     "shell.execute_reply": "2025-09-10T13:40:10.710398Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.689589Z"
    },
    "id": "2WevqkzDio8e",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device, text_processor, epoch, total_epochs):\n",
    "    \"\"\"Complete validation epoch with comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    metrics = OCRMetrics(text_processor)\n",
    "    batch_times = []\n",
    "\n",
    "    # Progress bar for validation\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation Epoch {epoch}/{total_epochs}\",\n",
    "                       leave=True, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get batch data\n",
    "            images = batch['images'].to(device)\n",
    "            encoded_texts = batch['encoded_texts'].to(device)\n",
    "            text_lengths = batch['text_lengths'].to(device)\n",
    "            original_texts = batch['texts']\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images) # Shape: (B, W, vocab_size)\n",
    "\n",
    "\n",
    "            # Calculate loss\n",
    "            batch_size = images.size(0)\n",
    "            # seq_length = logits.size(0) # Incorrect for CTC loss\n",
    "            seq_length = logits.size(1) # Correct sequence length (width W)\n",
    "\n",
    "            max_target_length = encoded_texts.size(1)\n",
    "            mask = torch.arange(max_target_length).expand(batch_size, max_target_length).to(device) < text_lengths.unsqueeze(1)\n",
    "            targets = encoded_texts[mask]\n",
    "            target_lengths = text_lengths\n",
    "            input_lengths = torch.full(size=(batch_size,), fill_value=seq_length, dtype=torch.long).to(device)\n",
    "\n",
    "            # log_probs = F.log_softmax(logits, dim=2) # Shape: (B, W, vocab_size)\n",
    "            # Transpose log_probs to (W, B, vocab_size) for CTCLoss\n",
    "            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
    "\n",
    "            loss = criterion(log_probs, targets.long(), input_lengths, target_lengths.long())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_time = time.time() - start_time\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "            # Decode predictions - logits shape expected by ctc_decode is (T, B, C)\n",
    "            # Need to transpose logits back to (W, B, vocab_size) before decoding\n",
    "            decoded_texts = ctc_decode(logits.transpose(0, 1), text_processor)\n",
    "            metrics.update(decoded_texts, original_texts)\n",
    "\n",
    "            # Update progress bar\n",
    "            current_metrics = metrics.get_metrics()\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}',\n",
    "                'CER': f'{current_metrics.get(\"avg_cer\", 0):.4f}',\n",
    "                'Acc': f'{current_metrics.get(\"sequence_accuracy\", 0):.4f}',\n",
    "                'Time': f'{batch_time:.3f}s'\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    final_metrics = metrics.get_metrics()\n",
    "\n",
    "    return avg_loss, final_metrics, avg_batch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.712565Z",
     "iopub.status.busy": "2025-09-10T13:40:10.712244Z",
     "iopub.status.idle": "2025-09-10T13:40:10.733575Z",
     "shell.execute_reply": "2025-09-10T13:40:10.732707Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.712546Z"
    },
    "id": "OGZOaQ-wis6S",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device, text_processor):\n",
    "    \"\"\"Comprehensive testing with detailed metrics\"\"\"\n",
    "    print(\"Starting comprehensive testing...\")\n",
    "    model.eval()\n",
    "\n",
    "    metrics = OCRMetrics(text_processor)\n",
    "    total_loss = 0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    # Detailed analysis storage\n",
    "    error_analysis = {\n",
    "        'correct_samples': [],\n",
    "        'error_samples': [],\n",
    "        'cer_distribution': [],\n",
    "        'wer_distribution': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Testing\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            images = batch['images'].to(device)\n",
    "            encoded_texts = batch['encoded_texts'].to(device)\n",
    "            text_lengths = batch['text_lengths'].to(device)\n",
    "            original_texts = batch['texts']\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            batch_size = images.size(0)\n",
    "            # seq_length = logits.size(0) # Incorrect for CTC loss\n",
    "            seq_length = logits.size(1) # Correct sequence length (width W)\n",
    "\n",
    "            max_target_length = encoded_texts.size(1)\n",
    "            mask = torch.arange(max_target_length).expand(batch_size, max_target_length).to(device) < text_lengths.unsqueeze(1)\n",
    "            targets = encoded_texts[mask]\n",
    "            target_lengths = text_lengths\n",
    "            input_lengths = torch.full(size=(batch_size,), fill_value=seq_length, dtype=torch.long).to(device)\n",
    "\n",
    "            # log_probs = F.log_softmax(logits, dim=2) # Shape: (B, W, vocab_size)\n",
    "            # Transpose log_probs to (W, B, vocab_size) for CTCLoss\n",
    "            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)\n",
    "\n",
    "            loss = criterion(log_probs, targets.long(), input_lengths, target_lengths.long())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Decode predictions - logits shape expected by ctc_decode is (T, B, C)\n",
    "            # Need to transpose logits back to (W, B, vocab_size) before decoding\n",
    "            decoded_texts = ctc_decode(logits.transpose(0, 1), text_processor)\n",
    "            metrics.update(decoded_texts, original_texts)\n",
    "\n",
    "            # Error analysis\n",
    "            for pred, target in zip(decoded_texts, original_texts):\n",
    "                cer = editdistance.eval(pred, target) / max(len(target), 1)\n",
    "                wer = editdistance.eval(pred.split(), target.split()) / max(len(target.split()), 1)\n",
    "\n",
    "                error_analysis['cer_distribution'].append(cer)\n",
    "                error_analysis['wer_distribution'].append(wer)\n",
    "\n",
    "                if pred == target:\n",
    "                    error_analysis['correct_samples'].append((pred, target))\n",
    "                else:\n",
    "                    error_analysis['error_samples'].append((pred, target))\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    final_metrics = metrics.get_metrics()\n",
    "\n",
    "    return avg_loss, final_metrics, error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:40:10.735007Z",
     "iopub.status.busy": "2025-09-10T13:40:10.734656Z",
     "iopub.status.idle": "2025-09-10T13:40:10.758299Z",
     "shell.execute_reply": "2025-09-10T13:40:10.757408Z",
     "shell.execute_reply.started": "2025-09-10T13:40:10.734982Z"
    },
    "id": "5F-Z6tFKiv9C",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_complete_pipeline(model, train_loader, val_loader, test_loader, text_processor,\n",
    "                          config, device, save_dir='./checkpoints'):\n",
    "    \"\"\"Complete training pipeline with all features and visualizations\"\"\"\n",
    "\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize components\n",
    "    criterion = CTCLoss(blank=text_processor.char2idx[text_processor.BLANK_TOKEN],\n",
    "                       reduction='mean', zero_infinity=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'],\n",
    "                     weight_decay=config['weight_decay'])\n",
    "\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs'],\n",
    "                                 eta_min=config['learning_rate'] * 0.01)\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_metrics': [],\n",
    "        'val_loss': [], 'val_metrics': [],\n",
    "        'best_val_cer': float('inf'), 'best_epoch': 0,\n",
    "        'epochs_no_improve': 0 # Counter for early stopping\n",
    "    }\n",
    "\n",
    "    print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Get model saving parameters from config\n",
    "    best_model_filename = config.get('best_model_filename', 'best_model.pth')\n",
    "    regular_checkpoint_frequency = config.get('regular_checkpoint_frequency', 5)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{config['num_epochs']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_metrics, train_time = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler,\n",
    "            device, text_processor, epoch, config['num_epochs'],\n",
    "            use_amp=True, grad_clip=config['gradient_clip']\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_metrics, val_time = validate_epoch(\n",
    "            model, val_loader, criterion, device, text_processor,\n",
    "            epoch, config['num_epochs']\n",
    "        )\n",
    "\n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_metrics'].append(train_metrics)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_metrics'].append(val_metrics)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train CER: {train_metrics['avg_cer']:.4f} | Val CER: {val_metrics['avg_cer']:.4f}\")\n",
    "        print(f\"Train Acc: {train_metrics['sequence_accuracy']:.4f} | Val Acc: {val_metrics['sequence_accuracy']:.4f}\")\n",
    "        print(f\"Train Time: {train_time:.2f}s | Val Time: {val_time:.2f}s\")\n",
    "\n",
    "\n",
    "        # Save best model and check for early stopping\n",
    "        current_val_cer = val_metrics['avg_cer']\n",
    "        if current_val_cer < history['best_val_cer']:\n",
    "            history['best_val_cer'] = current_val_cer\n",
    "            history['best_epoch'] = epoch\n",
    "            history['epochs_no_improve'] = 0 # Reset counter\n",
    "\n",
    "            # Save model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'config': config,\n",
    "                'val_cer': current_val_cer,\n",
    "                'text_processor': text_processor\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, os.path.join(save_dir, best_model_filename))\n",
    "            print(f\"✓ New best model saved with CER: {current_val_cer:.4f}\")\n",
    "        else:\n",
    "            history['epochs_no_improve'] += 1\n",
    "            print(f\"Validation CER did not improve. Epochs with no improvement: {history['epochs_no_improve']}/{config['early_stopping_patience']}\")\n",
    "\n",
    "\n",
    "        # Save regular checkpoint based on configurable frequency\n",
    "        if epoch % regular_checkpoint_frequency == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'config': config,\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "        # Check for early stopping\n",
    "        if history['epochs_no_improve'] >= config['early_stopping_patience']:\n",
    "            print(f\"\\nEarly stopping triggered after {config['early_stopping_patience']} epochs with no improvement on validation CER.\")\n",
    "            break # Exit the training loop\n",
    "\n",
    "    # --- Post-training Analysis and Visualization ---\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load best model for final evaluation and inference\n",
    "    best_checkpoint_path = os.path.join(save_dir, best_model_filename)\n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        try:\n",
    "            # Load the checkpoint with weights_only=False to access other saved data\n",
    "            best_checkpoint = torch.load(best_checkpoint_path, map_location=device, weights_only=False)\n",
    "            model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded best model from epoch {best_checkpoint['epoch']} for final evaluation.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model checkpoint from {best_checkpoint_path}: {e}\")\n",
    "            print(\"Proceeding with the model from the last training epoch.\")\n",
    "    else:\n",
    "        print(f\"Best model checkpoint not found at {best_checkpoint_path}.\")\n",
    "        print(\"Proceeding with the model from the last training epoch.\")\n",
    "\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_metrics, error_analysis_test = test_model(\n",
    "        model, test_loader, criterion, device, text_processor\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set for detailed analysis\n",
    "    val_loss_detailed, val_metrics_detailed, error_analysis_val = test_model(\n",
    "         model, val_loader, criterion, device, text_processor\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create inference pipeline with the potentially best loaded model\n",
    "    inference_pipeline = ArabicOCRInference(model, text_processor, device)\n",
    "\n",
    "\n",
    "    return history, test_metrics, error_analysis_test, val_metrics_detailed, error_analysis_val, inference_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DwdYp2vj2_Q"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-10T13:41:08.925Z",
     "iopub.execute_input": "2025-09-10T13:40:10.759667Z",
     "iopub.status.busy": "2025-09-10T13:40:10.759319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SVTR(\n",
    "    img_size=(config['img_height'], config['img_width']),\n",
    "    in_chans=config['channels'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    local_type=config['local_type'],  # Use the list from config\n",
    "    embed_dims=config['embed_dims'],  # Use embed_dims from config\n",
    "    heads=config['heads'],  # Use heads from config\n",
    "    mlp_ratio=config['mlp_ratio'],  # Use mlp_ratio from config\n",
    "    window_sizes=config['window_sizes'],  # Use window_sizes from config\n",
    "    num_blocks=config['num_blocks'],  # Use num_blocks from config\n",
    "    pattern=config['pattern'],  # Use pattern from config\n",
    "    drop=config['dropout_rate'],  # Use dropout_rate from config\n",
    "    n_points=config['n_points'],  # Use n_points from config\n",
    "    offset_scale=config['offset_scale'],  # Use offset_scale from config\n",
    ").to(config['device'])\n",
    "\n",
    "\n",
    "# Add option to load a previously trained model\n",
    "if 'load_model_path' in config and config['load_model_path']:\n",
    "    model_path = config['load_model_path']\n",
    "    exclude_head = config.get('exclude_head_on_load', True)  # default True for safety\n",
    "    print(f\"Loading model from: {model_path} (exclude_head={exclude_head})\")\n",
    "    try:\n",
    "        # Check if the path is a directory and look for a model file\n",
    "        if os.path.isdir(model_path):\n",
    "            print(f\"'{model_path}' is a directory. Searching for a model file inside...\")\n",
    "            # Assuming the model file is a .pth file within the directory\n",
    "            model_file = None\n",
    "            for f in os.listdir(model_path):\n",
    "                if f.endswith('.pth') or f.endswith('.pt'):\n",
    "                    model_file = os.path.join(model_path, f)\n",
    "                    break\n",
    "            if model_file:\n",
    "                print(f\"Found model file: {model_file}\")\n",
    "                # Load the checkpoint\n",
    "                checkpoint = torch.load(model_file, map_location=config['device'], weights_only=False)\n",
    "\n",
    "                # Get the state dict from the checkpoint\n",
    "                pretrained_state_dict = checkpoint['model_state_dict']\n",
    "                model_state_dict = model.state_dict()\n",
    "\n",
    "                # Filter out unwanted keys if exclude_head=True\n",
    "                if exclude_head:\n",
    "                    new_state_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                                      if k in model_state_dict and not k.startswith('head.')}\n",
    "                else:\n",
    "                    new_state_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                                      if k in model_state_dict}\n",
    "\n",
    "                # Update and load\n",
    "                model_state_dict.update(new_state_dict)\n",
    "                model.load_state_dict(model_state_dict)\n",
    "\n",
    "                print(\"Model loaded successfully!\")\n",
    "            else:\n",
    "                print(f\"Error: No .pth or .pt model file found in directory {model_path}\")\n",
    "        else:\n",
    "            # If it's not a directory, try loading it directly\n",
    "            checkpoint = torch.load(model_path, map_location=config['device'], weights_only=False)\n",
    "            pretrained_state_dict = checkpoint['model_state_dict']\n",
    "            model_state_dict = model.state_dict()\n",
    "\n",
    "            if exclude_head:\n",
    "                new_state_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                                  if k in model_state_dict and not k.startswith('head.')}\n",
    "            else:\n",
    "                new_state_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "                                  if k in model_state_dict}\n",
    "\n",
    "            model_state_dict.update(new_state_dict)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            print(\"Model loaded successfully!\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file or directory not found at {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "# Train the complete pipeline\n",
    "history, test_metrics, error_analysis_test, val_metrics_detailed, error_analysis_val, inference_pipeline = train_complete_pipeline(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    text_processor=text_processor,\n",
    "    config=config,\n",
    "    device=config['device'],\n",
    "    save_dir=config['save_path_directory']\n",
    ")\n",
    "\n",
    "# --- Utilization of pipeline outputs ---\n",
    "\n",
    "# Visualize training history (loss and metrics)\n",
    "plot_training_history(history)\n",
    "\n",
    "# Display final test metrics\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "for metric_name, value in test_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Display detailed error analysis for validation set\n",
    "display_error_analysis(error_analysis_val, dataset_name=\"Validation Set\")\n",
    "plot_error_distributions(error_analysis_val, dataset_name=\"Validation Set\")\n",
    "\n",
    "# Display detailed error analysis for test set\n",
    "display_error_analysis(error_analysis_test, dataset_name=\"Test Set\")\n",
    "plot_error_distributions(error_analysis_test, dataset_name=\"Test Set\")\n",
    "\n",
    "# Visualize samples with predictions from the test loader using the inference pipeline\n",
    "visualize_predictions(test_loader, text_processor, inference_pipeline, num_samples=12,\n",
    "                      dataset_mean=config['dataset_mean'], dataset_std=config['dataset_std'])\n",
    "\n",
    "\n",
    "# Example inference usage with the returned inference_pipeline\n",
    "print(\"\\nExample Inference using the returned pipeline:\")\n",
    "all_inference_labels = val_dataset.labels + test_dataset.labels\n",
    "num_inference_samples = 32  # Adjust this number as needed\n",
    "random.shuffle(all_inference_labels)\n",
    "inference_samples = all_inference_labels[:min(num_inference_samples, len(all_inference_labels))]\n",
    "\n",
    "if inference_samples:\n",
    "    print(f\"Performing inference on {len(inference_samples)} random samples from validation and test sets:\")\n",
    "    inference_images_paths = [img_path for img_path, _ in inference_samples]\n",
    "    actual_texts = [text for _, text in inference_samples]\n",
    "\n",
    "    predicted_texts = inference_pipeline.predict_batch(inference_images_paths)\n",
    "\n",
    "    print(\"\\nInference Results:\")\n",
    "    for i in range(len(inference_samples)):\n",
    "        actual_text = actual_texts[i]\n",
    "        predicted_text = predicted_texts[i]\n",
    "        display_actual_text = actual_text\n",
    "        display_predicted_text = predicted_text\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Actual:    '{display_actual_text}'\")\n",
    "        print(f\"  Predicted: '{display_predicted_text}'\")\n",
    "        print(f\"  Correct:   {actual_text == predicted_text}\")\n",
    "        print(\"-\" * 20)\n",
    "else:\n",
    "    print(\"No samples available in validation or test dataset for inference example.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WrsFAGE2T6Ui",
    "xr3P87LS4VhE",
    "Y2F7OlGd4Yob",
    "d9lu5wm_Xq9d",
    "kQdwj3sLUXm-",
    "Y-u-JDfs2NVi",
    "BhEnDOXL3y1L",
    "bf4J7HcQ_aRH",
    "UehGzcWVn2x4",
    "dKNYODzyoEby",
    "HZGtSKJPob9B",
    "qXboeg6-6CG6",
    "ZfkGYkOghYAK",
    "Pc0sTuOXjkS1",
    "LVivMR8EkhLw",
    "F_4h8--E8Az-",
    "s8QMwgfT8uvn",
    "O-NMywQZ9wYv",
    "AsRiDExd-ec3",
    "nJ3iWvDEhqhe",
    "yqYwIAr1huef"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8190590,
     "sourceId": 12942859,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8238219,
     "sourceId": 13012435,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 446980,
     "modelInstanceId": 430036,
     "sourceId": 574514,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
